{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d560502",
   "metadata": {},
   "source": [
    "# First Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6949229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import gc\n",
    "import os\n",
    "\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tensors\n",
    "train_inputs_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_inputs_norm.pt').to(device)\n",
    "train_targets_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_targets_norm.pt').to(device)\n",
    "print(f\"Tensors loaded: train inputs {train_inputs_tensor.shape}, train targets {train_targets_tensor.shape}\")\n",
    "print(f\"Normalized t2m_f mean: {train_targets_tensor[:, :, 5, :].mean().item():.2f}, std: {train_targets_tensor[:, :, 5, :].std().item():.2f}\")\n",
    "\n",
    "# Temporary Train/Test split\n",
    "train_size = int(0.9 * train_inputs_tensor.shape[0])\n",
    "print(f\"Train size: {train_size}\")\n",
    "temp_train_inputs = train_inputs_tensor[:train_size]\n",
    "temp_train_targets = train_targets_tensor[:train_size]\n",
    "temp_test_inputs = train_inputs_tensor[train_size:]\n",
    "temp_test_targets = train_targets_tensor[train_size:]\n",
    "print(f\"Train/test split: train shape {temp_train_inputs.shape}, test shape {temp_test_inputs.shape}\")\n",
    "\n",
    "# Move tensors to GPU upfront\n",
    "temp_train_inputs = temp_train_inputs.to(device)\n",
    "temp_train_targets = temp_train_targets.to(device)\n",
    "temp_test_inputs = temp_test_inputs.to(device)\n",
    "temp_test_targets = temp_test_targets.to(device)\n",
    "\n",
    "# Define k values to test\n",
    "k_values = [4, 8, 24]\n",
    "results = {}  # Store MAE and RMSE for each k\n",
    "\n",
    "# Coordinates for edge index (computed once)\n",
    "num_nodes = 23937\n",
    "lat_subset = np.linspace(50, 25, 101)\n",
    "lon_subset = np.linspace(235, 294, 237)\n",
    "coords = np.stack(np.meshgrid(lat_subset, lon_subset, indexing='ij'), axis=-1).reshape(-1, 2)\n",
    "\n",
    "# Model and loss functions remain the same\n",
    "class WeatherGNN(t.nn.Module):\n",
    "    def __init__(self, num_features=15, num_outputs=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, 256)\n",
    "        self.conv2 = GCNConv(256, num_outputs)\n",
    "        self.dropout = t.nn.Dropout(0.3)\n",
    "        self.residual = t.nn.Linear(num_features, num_outputs)\n",
    "        self.res_weight = t.nn.Parameter(t.tensor(2.0))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        residual = self.residual(x) * self.res_weight\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x + residual\n",
    "\n",
    "loss_fns = {\n",
    "    'L1': lambda x, y: t.mean(t.abs(x - y)),\n",
    "    'L2': lambda x, y: t.mean((x - y) ** 2),\n",
    "    'Huber': nn.SmoothL1Loss(reduction='mean')\n",
    "}\n",
    "\n",
    "# Pre-compute t2m_f statistics for denormalization\n",
    "t2m_f_mean = 42.36  # °F\n",
    "t2m_f_std = 21.75   # °F\n",
    "\n",
    "# Loop over k values\n",
    "for k in k_values:\n",
    "    print(f\"\\nTesting with k={k} neighbors...\")\n",
    "    \n",
    "    # Compute edge index for current k\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1).fit(coords)\n",
    "    _, indices = nbrs.kneighbors(coords)\n",
    "    edge_index = t.tensor(np.stack([np.repeat(np.arange(num_nodes), k), indices[:, 1:].flatten()]), dtype=t.long).to(device)\n",
    "    print(f\"Edge index computed for k={k}, shape: {edge_index.shape}\")\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    num_epochs = 10\n",
    "    val_steps = list(range(0, temp_test_inputs.shape[0] - 1, 2))\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for loss_name, criterion in loss_fns.items():\n",
    "        print(f\"\\nTraining with {loss_name} loss for t2m_f prediction (k={k})...\")\n",
    "        t.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        model = WeatherGNN(num_features=15, num_outputs=1).to(device)\n",
    "        optimizer = t.optim.Adam(model.parameters(), lr=0.01)\n",
    "        scheduler = t.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
    "        train_losses, val_losses = [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start = time.time()\n",
    "            print(f\"Starting Epoch {epoch+1}/{num_epochs}\")\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for t_step in range(temp_train_inputs.shape[0] - 1):\n",
    "                input_x = temp_train_inputs[t_step].reshape(num_nodes, -1)\n",
    "                target_y = temp_train_targets[t_step, :, 5, :].reshape(num_nodes, -1)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(input_x, edge_index)\n",
    "                loss = criterion(out, target_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            scheduler.step()\n",
    "            avg_loss = total_loss / (temp_train_inputs.shape[0] - 1)\n",
    "            train_losses.append(avg_loss)\n",
    "            epoch_time = time.time() - epoch_start\n",
    "            print(f\"Finished Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}, Total runtime: {epoch_time:.2f}s\")\n",
    "\n",
    "            if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "                val_start = time.time()\n",
    "                print(f\"Starting Validation for Epoch {epoch+1}\")\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                with t.no_grad():\n",
    "                    for t_step in val_steps:\n",
    "                        input_x = temp_test_inputs[t_step].reshape(num_nodes, -1)\n",
    "                        target_y = temp_test_targets[t_step, :, 5, :].reshape(num_nodes, -1)\n",
    "                        out = model(input_x, edge_index)\n",
    "                        val_loss += criterion(out, target_y).item()\n",
    "                val_loss = val_loss / len(val_steps)\n",
    "                val_losses.append(val_loss)\n",
    "                val_time = time.time() - val_start\n",
    "                print(f\"Finished Validation for Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Total runtime: {val_time:.2f}s\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    best_model_state = model.state_dict()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}: Validation loss has not improved for {patience} validations.\")\n",
    "                        break\n",
    "\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "            print(\"Loaded best model state based on validation loss.\")\n",
    "\n",
    "        # Test evaluation\n",
    "        test_start = time.time()\n",
    "        print(f\"Starting Final Test Evaluation for {loss_name} loss (k={k})\")\n",
    "        model.eval()\n",
    "        test_preds, test_trues = [], []\n",
    "        with t.no_grad():\n",
    "            for t_step in range(temp_test_inputs.shape[0] - 1):\n",
    "                input_x = temp_test_inputs[t_step].reshape(num_nodes, -1)\n",
    "                target_y = temp_test_targets[t_step, :, 5, :].reshape(num_nodes, -1)\n",
    "                out = model(input_x, edge_index)\n",
    "                test_preds.append(out.cpu())\n",
    "                test_trues.append(target_y.cpu())\n",
    "        test_preds, test_trues = t.stack(test_preds), t.stack(test_trues)\n",
    "        test_time = time.time() - test_start\n",
    "        print(f\"Finished Final Test Evaluation for {loss_name} loss, Total runtime: {test_time:.2f}s\")\n",
    "\n",
    "        # Denormalize and evaluate t2m_f\n",
    "        preds_t2m = test_preds * t2m_f_std + t2m_f_mean\n",
    "        trues_t2m = test_trues * t2m_f_std + t2m_f_mean\n",
    "        mae_t2m = t.mean(t.abs(preds_t2m - trues_t2m)).item()\n",
    "        rmse_t2m = t.sqrt(t.mean((preds_t2m - trues_t2m) ** 2)).item()\n",
    "        print(f\"t2m_f L1 norm (°F) (k={k}): {mae_t2m:.2f}\")\n",
    "        print(f\"t2m_f RMSE (°F) (k={k}): {rmse_t2m:.2f}\")\n",
    "\n",
    "        # Store results\n",
    "        results[(k, loss_name)] = {'MAE': mae_t2m, 'RMSE': rmse_t2m}\n",
    "\n",
    "        # Denormalize sample outputs\n",
    "        sample_preds = preds_t2m[:5, 0, 0].numpy()\n",
    "        sample_targets = trues_t2m[:5, 0, 0].numpy()\n",
    "        print(f\"Sample t2m_f preds (°F) (k={k}): {sample_preds}\")\n",
    "        print(f\"Sample t2m_f targets (°F) (k={k}): {sample_targets}\")\n",
    "\n",
    "        # Loss Plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, len(train_losses) + 1), train_losses, label=f'Train Loss ({loss_name})', color='blue')\n",
    "        plt.plot([i * 2 for i in range(len(val_losses))], val_losses, label='Validation Loss', color='orange', marker='o')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(f'{loss_name} Loss (Normalized)')\n",
    "        plt.title(f'Training and Validation Loss for t2m_f ({loss_name}, k={k})')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'loss_plot_t2m_f_{loss_name}_k{k}.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Time Series Plot for t2m_f\n",
    "        plot_start = time.time()\n",
    "        print(f\"Starting Time Series Plotting for t2m_f (k={k})\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(preds_t2m[:, 0, 0].numpy(), label='Pred t2m_f', color='red', linestyle='--')\n",
    "        plt.plot(trues_t2m[:, 0, 0].numpy(), label='True t2m_f', color='green')\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Temperature (°F)')\n",
    "        plt.title(f'Predicted vs Actual t2m_f (k={k})')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f't2m_f_timeseries_k{k}.png')\n",
    "        print(f\"Finished Time Series Plotting, Total runtime: {time.time() - plot_start:.2f}s\")\n",
    "        plt.show()\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\nSummary of Results:\")\n",
    "for (k, loss_name), metrics in results.items():\n",
    "    print(f\"k={k}, Loss={loss_name}: MAE={metrics['MAE']:.2f}°F, RMSE={metrics['RMSE']:.2f}°F\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b452115b",
   "metadata": {},
   "source": [
    "# Second Iteration: Adding a third GCN Convolution layer to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a49dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import gc\n",
    "import os\n",
    "\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tensors\n",
    "train_inputs_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_inputs_norm.pt').to(device)\n",
    "train_targets_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_targets_norm.pt').to(device)\n",
    "print(f\"Tensors loaded: train inputs {train_inputs_tensor.shape}, train targets {train_targets_tensor.shape}\")\n",
    "print(f\"Normalized t2m_f mean: {train_targets_tensor[:, :, 5, :].mean().item():.2f}, std: {train_targets_tensor[:, :, 5, :].std().item():.2f}\")\n",
    "\n",
    "# Temporary Train/Test split\n",
    "train_size = int(0.9 * train_inputs_tensor.shape[0])\n",
    "print(f\"Train size: {train_size}\")\n",
    "temp_train_inputs = train_inputs_tensor[:train_size]\n",
    "temp_train_targets = train_targets_tensor[:train_size]\n",
    "temp_test_inputs = train_inputs_tensor[train_size:]\n",
    "temp_test_targets = train_targets_tensor[train_size:]\n",
    "print(f\"Train/test split: train shape {temp_train_inputs.shape}, test shape {temp_test_inputs.shape}\")\n",
    "\n",
    "# Move tensors to GPU upfront\n",
    "temp_train_inputs = temp_train_inputs.to(device)\n",
    "temp_train_targets = temp_train_targets.to(device)\n",
    "temp_test_inputs = temp_test_inputs.to(device)\n",
    "temp_test_targets = temp_test_targets.to(device)\n",
    "\n",
    "# Define k values to test\n",
    "k_values = [4, 8, 24]\n",
    "results = {}  # Store MAE and RMSE for each k\n",
    "\n",
    "# Coordinates for edge index (computed once)\n",
    "num_nodes = 23937\n",
    "lat_subset = np.linspace(50, 25, 101)\n",
    "lon_subset = np.linspace(235, 294, 237)\n",
    "coords = np.stack(np.meshgrid(lat_subset, lon_subset, indexing='ij'), axis=-1).reshape(-1, 2)\n",
    "\n",
    "# Updated model with three GCNConv layers\n",
    "class WeatherGNN(t.nn.Module):\n",
    "    def __init__(self, num_features=15, num_outputs=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, 128)  # Lowered hidden_dim\n",
    "        self.conv2 = GCNConv(128, 128)\n",
    "        self.conv3 = GCNConv(128, num_outputs)  # Added third layer\n",
    "        self.dropout = t.nn.Dropout(0.3)  # Increased dropout\n",
    "        self.residual = t.nn.Linear(num_features, num_outputs)\n",
    "        self.res_weight = t.nn.Parameter(t.tensor(2.0))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        residual = self.residual(x) * self.res_weight\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x + residual\n",
    "\n",
    "loss_fns = {\n",
    "    'L1': lambda x, y: t.mean(t.abs(x - y)),\n",
    "    'L2': lambda x, y: t.mean((x - y) ** 2),\n",
    "    'Huber': nn.SmoothL1Loss(reduction='mean')\n",
    "}\n",
    "\n",
    "# Pre-compute t2m_f statistics for denormalization\n",
    "t2m_f_mean = 42.36  # °F\n",
    "t2m_f_std = 21.75   # °F\n",
    "\n",
    "# Loop over k values\n",
    "for k in k_values:\n",
    "    print(f\"\\nTesting with k={k} neighbors...\")\n",
    "    \n",
    "    # Compute edge index for current k\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1).fit(coords)\n",
    "    _, indices = nbrs.kneighbors(coords)\n",
    "    edge_index = t.tensor(np.stack([np.repeat(np.arange(num_nodes), k), indices[:, 1:].flatten()]), dtype=t.long).to(device)\n",
    "    print(f\"Edge index computed for k={k}, shape: {edge_index.shape}\")\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    num_epochs = 10\n",
    "    val_steps = list(range(0, temp_test_inputs.shape[0] - 1, 2))\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for loss_name, criterion in loss_fns.items():\n",
    "        print(f\"\\nTraining with {loss_name} loss for t2m_f prediction (k={k})...\")\n",
    "        t.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        model = WeatherGNN(num_features=15, num_outputs=1).to(device)\n",
    "        optimizer = t.optim.Adam(model.parameters(), lr=0.01)\n",
    "        scheduler = t.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
    "        train_losses, val_losses = [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start = time.time()\n",
    "            print(f\"Starting Epoch {epoch+1}/{num_epochs}\")\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for t_step in range(temp_train_inputs.shape[0] - 1):\n",
    "                input_x = temp_train_inputs[t_step].reshape(num_nodes, -1)\n",
    "                target_y = temp_train_targets[t_step, :, 5, :].reshape(num_nodes, -1)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(input_x, edge_index)\n",
    "                loss = criterion(out, target_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            scheduler.step()\n",
    "            avg_loss = total_loss / (temp_train_inputs.shape[0] - 1)\n",
    "            train_losses.append(avg_loss)\n",
    "            epoch_time = time.time() - epoch_start\n",
    "            print(f\"Finished Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}, Total runtime: {epoch_time:.2f}s\")\n",
    "\n",
    "            if epoch % 2 == 0 or epoch == num_epochs - 1:\n",
    "                val_start = time.time()\n",
    "                print(f\"Starting Validation for Epoch {epoch+1}\")\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                with t.no_grad():\n",
    "                    for t_step in val_steps:\n",
    "                        input_x = temp_test_inputs[t_step].reshape(num_nodes, -1)\n",
    "                        target_y = temp_test_targets[t_step, :, 5, :].reshape(num_nodes, -1)\n",
    "                        out = model(input_x, edge_index)\n",
    "                        val_loss += criterion(out, target_y).item()\n",
    "                val_loss = val_loss / len(val_steps)\n",
    "                val_losses.append(val_loss)\n",
    "                val_time = time.time() - val_start\n",
    "                print(f\"Finished Validation for Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Total runtime: {val_time:.2f}s\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    best_model_state = model.state_dict()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}: Validation loss has not improved for {patience} validations.\")\n",
    "                        break\n",
    "\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "            print(\"Loaded best model state based on validation loss.\")\n",
    "\n",
    "        # Test evaluation\n",
    "        test_start = time.time()\n",
    "        print(f\"Starting Final Test Evaluation for {loss_name} loss (k={k})\")\n",
    "        model.eval()\n",
    "        test_preds, test_trues = [], []\n",
    "        with t.no_grad():\n",
    "            for t_step in range(temp_test_inputs.shape[0] - 1):\n",
    "                input_x = temp_test_inputs[t_step].reshape(num_nodes, -1)\n",
    "                target_y = temp_test_targets[t_step, :, 5, :].reshape(num_nodes, -1)\n",
    "                out = model(input_x, edge_index)\n",
    "                test_preds.append(out.cpu())\n",
    "                test_trues.append(target_y.cpu())\n",
    "        test_preds, test_trues = t.stack(test_preds), t.stack(test_trues)\n",
    "        test_time = time.time() - test_start\n",
    "        print(f\"Finished Final Test Evaluation for {loss_name} loss, Total runtime: {test_time:.2f}s\")\n",
    "\n",
    "        # Denormalize and evaluate t2m_f\n",
    "        preds_t2m = test_preds * t2m_f_std + t2m_f_mean\n",
    "        trues_t2m = test_trues * t2m_f_std + t2m_f_mean\n",
    "        mae_t2m = t.mean(t.abs(preds_t2m - trues_t2m)).item()\n",
    "        rmse_t2m = t.sqrt(t.mean((preds_t2m - trues_t2m) ** 2)).item()\n",
    "        print(f\"t2m_f L1 norm (°F) (k={k}): {mae_t2m:.2f}\")\n",
    "        print(f\"t2m_f RMSE (°F) (k={k}): {rmse_t2m:.2f}\")\n",
    "\n",
    "        # Store results\n",
    "        results[(k, loss_name)] = {'MAE': mae_t2m, 'RMSE': rmse_t2m}\n",
    "\n",
    "        # Denormalize sample outputs\n",
    "        sample_preds = preds_t2m[:5, 0, 0].numpy()\n",
    "        sample_targets = trues_t2m[:5, 0, 0].numpy()\n",
    "        print(f\"Sample t2m_f preds (°F) (k={k}): {sample_preds}\")\n",
    "        print(f\"Sample t2m_f targets (°F) (k={k}): {sample_targets}\")\n",
    "\n",
    "        # Loss Plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, len(train_losses) + 1), train_losses, label=f'Train Loss ({loss_name})', color='blue')\n",
    "        plt.plot([i * 2 for i in range(len(val_losses))], val_losses, label='Validation Loss', color='orange', marker='o')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(f'{loss_name} Loss (Normalized)')\n",
    "        plt.title(f'Training and Validation Loss for t2m_f ({loss_name}, k={k})')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'loss_plot_t2m_f_{loss_name}_k{k}.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Time Series Plot for t2m_f\n",
    "        plot_start = time.time()\n",
    "        print(f\"Starting Time Series Plotting for t2m_f (k={k})\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(preds_t2m[:, 0, 0].numpy(), label='Pred t2m_f', color='red', linestyle='--')\n",
    "        plt.plot(trues_t2m[:, 0, 0].numpy(), label='True t2m_f', color='green')\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Temperature (°F)')\n",
    "        plt.title(f'Predicted vs Actual t2m_f (k={k})')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f't2m_f_timeseries_k{k}.png')\n",
    "        print(f\"Finished Time Series Plotting, Total runtime: {time.time() - plot_start:.2f}s\")\n",
    "        plt.show()\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\nSummary of Results:\")\n",
    "for (k, loss_name), metrics in results.items():\n",
    "    print(f\"k={k}, Loss={loss_name}: MAE={metrics['MAE']:.2f}°F, RMSE={metrics['RMSE']:.2f}°F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21baeb94",
   "metadata": {},
   "source": [
    "# Third Iteration: Replacing the third GCN layer with a GRU layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8644cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import gc\n",
    "import os\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tensors and squeeze last dimension\n",
    "train_inputs_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_inputs_norm.pt').squeeze(-1).to(device)\n",
    "train_targets_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_targets_norm.pt').squeeze(-1).to(device)\n",
    "print(f\"Tensors loaded: train inputs {train_inputs_tensor.shape}, train targets {train_targets_tensor.shape}\")\n",
    "print(f\"Normalized t2m_f mean: {train_targets_tensor[:, :, 5].mean().item():.2f}, std: {train_targets_tensor[:, :, 5].std().item():.2f}\")\n",
    "\n",
    "# Temporary Train/Test split (90/10)\n",
    "train_size = int(0.9 * train_inputs_tensor.shape[0])\n",
    "print(f\"Train size: {train_size}\")\n",
    "temp_train_inputs = train_inputs_tensor[:train_size]\n",
    "temp_train_targets = train_targets_tensor[:train_size]\n",
    "temp_test_inputs = train_inputs_tensor[train_size:]\n",
    "temp_test_targets = train_targets_tensor[train_size:]\n",
    "print(f\"Train/test split: train shape {temp_train_inputs.shape}, test shape {temp_test_inputs.shape}\")\n",
    "\n",
    "# Limit training data to first 100 time steps\n",
    "temp_train_inputs = temp_train_inputs[:100]\n",
    "temp_train_targets = temp_train_targets[:100]\n",
    "print(f\"Limited training data to first 100 time steps: train shape {temp_train_inputs.shape}\")\n",
    "\n",
    "# Move tensors to GPU upfront\n",
    "temp_train_inputs = temp_train_inputs.to(device)\n",
    "temp_train_targets = temp_train_targets.to(device)\n",
    "temp_test_inputs = temp_test_inputs.to(device)\n",
    "temp_test_targets = temp_test_targets.to(device)\n",
    "\n",
    "# Prepare data for batching\n",
    "batch_size = 8  # Reduced batch size\n",
    "seq_len = 5     # Reduced sequence length\n",
    "num_nodes = 23937\n",
    "num_train_steps = temp_train_inputs.shape[0] - 1\n",
    "num_batches = (num_train_steps - seq_len) // batch_size\n",
    "num_test_steps = temp_test_inputs.shape[0] - 1\n",
    "val_steps = list(range(0, max(1, num_test_steps - seq_len + 1), 2))\n",
    "print(f\"Batching: batch_size={batch_size}, seq_len={seq_len}, num_batches={num_batches}, val_steps={val_steps}\")\n",
    "\n",
    "# Mixed precision training setup\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "# Define k values to test\n",
    "k_values = [4, 8, 24]\n",
    "results = {}  # Store MAE and RMSE for each k\n",
    "\n",
    "# Coordinates for edge index (computed once)\n",
    "lat_subset = np.linspace(50, 25, 101)\n",
    "lon_subset = np.linspace(235, 294, 237)\n",
    "coords = np.stack(np.meshgrid(lat_subset, lon_subset, indexing='ij'), axis=-1).reshape(-1, 2)\n",
    "\n",
    "# WeatherGNNGRU model\n",
    "class WeatherGNNGRU(t.nn.Module):\n",
    "    def __init__(self, num_features=15, hidden_dim=128, gru_hidden=16, num_outputs=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.gru = nn.GRU(hidden_dim, gru_hidden, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(gru_hidden, num_outputs)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.residual = nn.Linear(num_features, num_outputs)\n",
    "        self.res_weight = nn.Parameter(t.tensor(2.0))\n",
    "\n",
    "    def forward(self, x, edge_index, batch_size, seq_len, num_nodes):\n",
    "        batch_size, seq_len, num_nodes, num_features = x.shape\n",
    "        x = x.view(batch_size * seq_len, num_nodes, num_features)\n",
    "        residual = self.residual(x).view(batch_size, seq_len, num_nodes, 1) * self.res_weight\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = x.view(batch_size, seq_len, num_nodes, -1)\n",
    "        x = x.mean(dim=2)\n",
    "        x, _ = self.gru(x)\n",
    "        out = self.fc(x)\n",
    "        out = out[:, -1, :].unsqueeze(-1).expand(-1, num_nodes, -1)\n",
    "        residual = residual[:, -1, :, :]\n",
    "        return out + residual\n",
    "\n",
    "loss_fns = {\n",
    "    'L1': lambda x, y: t.mean(t.abs(x - y)),\n",
    "    'L2': lambda x, y: t.mean((x - y) ** 2),\n",
    "    'Huber': nn.SmoothL1Loss(reduction='mean')\n",
    "}\n",
    "\n",
    "# Pre-compute t2m_f statistics for denormalization\n",
    "t2m_f_mean = 42.36  # °F\n",
    "t2m_f_std = 21.75   # °F\n",
    "\n",
    "# Loop over k values\n",
    "for k in k_values:\n",
    "    print(f\"\\nTesting with k={k} neighbors...\")\n",
    "    \n",
    "    # Compute edge index for current k\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1).fit(coords)\n",
    "    _, indices = nbrs.kneighbors(coords)\n",
    "    edge_index = t.tensor(np.stack([np.repeat(np.arange(num_nodes), k), indices[:, 1:].flatten()]), dtype=t.long).to(device)\n",
    "    print(f\"Edge index computed for k={k}, shape: {edge_index.shape}\")\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    num_epochs = 10\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for loss_name, criterion in loss_fns.items():\n",
    "        print(f\"\\nTraining with {loss_name} loss for t2m_f prediction (k={k})...\")\n",
    "        t.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        model = WeatherGNNGRU(num_features=15, hidden_dim=128, gru_hidden=16, num_outputs=1).to(device)\n",
    "        optimizer = t.optim.Adam(model.parameters(), lr=0.01)\n",
    "        scheduler = t.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
    "        train_losses, val_losses = [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start = time.time()\n",
    "            print(f\"Starting Epoch {epoch+1}/{num_epochs}\")\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            num_valid_batches = 0\n",
    "\n",
    "            for batch_idx in range(num_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "                if end_idx + seq_len > num_train_steps:\n",
    "                    continue\n",
    "                input_x = temp_train_inputs[start_idx:end_idx + seq_len].unfold(0, seq_len, 1).permute(0, 1, 3, 2)\n",
    "                target_y = temp_train_targets[start_idx:end_idx + seq_len, :, 5].unfold(0, seq_len, 1).unsqueeze(-1)\n",
    "                input_x = input_x.reshape(-1, seq_len, num_nodes, 15)\n",
    "                target_y = target_y.reshape(-1, seq_len, num_nodes, 1)\n",
    "                optimizer.zero_grad()\n",
    "                with autocast('cuda'):\n",
    "                    out = model(input_x, edge_index, input_x.shape[0], seq_len, num_nodes)\n",
    "                    loss = criterion(out, target_y[:, -1, :, :])\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "                num_valid_batches += 1\n",
    "\n",
    "            scheduler.step()\n",
    "            avg_loss = total_loss / max(num_valid_batches, 1)\n",
    "            train_losses.append(avg_loss)\n",
    "            epoch_time = time.time() - epoch_start\n",
    "            print(f\"Finished Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}, Total runtime: {epoch_time:.2f}s\")\n",
    "\n",
    "            if epoch % 2 == 0 or epoch == num_epochs - 1:\n",
    "                val_start = time.time()\n",
    "                print(f\"Starting Validation for Epoch {epoch+1}\")\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                num_valid_val_batches = 0\n",
    "                with t.no_grad():\n",
    "                    for t_step in val_steps:\n",
    "                        if t_step + seq_len > num_test_steps:\n",
    "                            continue\n",
    "                        input_x = temp_test_inputs[t_step:t_step + seq_len].unsqueeze(0)\n",
    "                        target_y = temp_test_targets[t_step:t_step + seq_len, :, 5].unsqueeze(0).unsqueeze(-1)\n",
    "                        with autocast('cuda'):\n",
    "                            out = model(input_x, edge_index, 1, seq_len, num_nodes)\n",
    "                            val_loss += criterion(out, target_y[:, -1, :, :]).item()\n",
    "                        num_valid_val_batches += 1\n",
    "                val_loss = val_loss / max(num_valid_val_batches, 1)\n",
    "                val_losses.append(val_loss)\n",
    "                val_time = time.time() - val_start\n",
    "                print(f\"Finished Validation for Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Total runtime: {val_time:.2f}s\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    best_model_state = model.state_dict()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}: Validation loss has not improved for {patience} validations.\")\n",
    "                        break\n",
    "\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "            print(\"Loaded best model state based on validation loss.\")\n",
    "\n",
    "        # Test evaluation\n",
    "        test_start = time.time()\n",
    "        print(f\"Starting Final Test Evaluation for {loss_name} loss (k={k})\")\n",
    "        model.eval()\n",
    "        test_preds, test_trues = [], []\n",
    "        with t.no_grad():\n",
    "            for t_step in range(num_test_steps - seq_len + 1):\n",
    "                input_x = temp_test_inputs[t_step:t_step + seq_len].unsqueeze(0)\n",
    "                target_y = temp_test_targets[t_step:t_step + seq_len, :, 5].unsqueeze(0).unsqueeze(-1)\n",
    "                with autocast('cuda'):\n",
    "                    out = model(input_x, edge_index, 1, seq_len, num_nodes)\n",
    "                test_preds.append(out[:, :, 0].cpu())  # Fix: Use 3D indexing\n",
    "                test_trues.append(target_y[:, -1, :, 0].cpu())\n",
    "        test_preds = t.cat(test_preds, dim=0)\n",
    "        test_trues = t.cat(test_trues, dim=0)\n",
    "        test_time = time.time() - test_start\n",
    "        print(f\"Finished Final Test Evaluation for {loss_name} loss, Total runtime: {test_time:.2f}s\")\n",
    "\n",
    "        # Denormalize and evaluate t2m_f\n",
    "        preds_t2m = test_preds * t2m_f_std + t2m_f_mean\n",
    "        trues_t2m = test_trues * t2m_f_std + t2m_f_mean\n",
    "        mae_t2m = t.mean(t.abs(preds_t2m - trues_t2m)).item()\n",
    "        rmse_t2m = t.sqrt(t.mean((preds_t2m - trues_t2m) ** 2)).item()\n",
    "        print(f\"t2m_f L1 norm (°F) (k={k}): {mae_t2m:.2f}\")\n",
    "        print(f\"t2m_f RMSE (°F) (k={k}): {rmse_t2m:.2f}\")\n",
    "\n",
    "        # Store results\n",
    "        results[(k, loss_name)] = {'MAE': mae_t2m, 'RMSE': rmse_t2m}\n",
    "\n",
    "        # Denormalize sample outputs\n",
    "        sample_preds = preds_t2m[:5, 0].numpy()\n",
    "        sample_targets = trues_t2m[:5, 0].numpy()\n",
    "        print(f\"Sample t2m_f preds (°F) (k={k}): {sample_preds}\")\n",
    "        print(f\"Sample t2m_f targets (°F) (k={k}): {sample_targets}\")\n",
    "\n",
    "        # Loss Plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, len(train_losses) + 1), train_losses, label=f'Train Loss ({loss_name})', color='blue')\n",
    "        plt.plot([i * 2 for i in range(len(val_losses))], val_losses, label='Validation Loss', color='orange', marker='o')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(f'{loss_name} Loss (Normalized)')\n",
    "        plt.title(f'Training and Validation Loss for t2m_f ({loss_name}, k={k})')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'loss_plot_t2m_f_{loss_name}_k{k}.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Time Series Plot for t2m_f\n",
    "        plot_start = time.time()\n",
    "        print(f\"Starting Time Series Plotting for t2m_f (k={k})\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(preds_t2m[:, 0].numpy(), label='Pred t2m_f', color='red', linestyle='--')\n",
    "        plt.plot(trues_t2m[:, 0].numpy(), label='True t2m_f', color='green')\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Temperature (°F)')\n",
    "        plt.title(f'Predicted vs Actual t2m_f (k={k})')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f't2m_f_timeseries_k{k}.png')\n",
    "        plt.show()\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\nSummary of Results:\")\n",
    "for (k, loss_name), metrics in results.items():\n",
    "    print(f\"k={k}, Loss={loss_name}: MAE={metrics['MAE']:.2f}°F, RMSE={metrics['RMSE']:.2f}°F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d0e619",
   "metadata": {},
   "source": [
    "## Cut batch size in half to fix memory allocation error for k=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import gc\n",
    "import os\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "# Set environment variable to reduce memory fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tensors and squeeze last dimension\n",
    "train_inputs_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_inputs_norm.pt').squeeze(-1).to(device)\n",
    "train_targets_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_targets_norm.pt').squeeze(-1).to(device)\n",
    "print(f\"Tensors loaded: train inputs {train_inputs_tensor.shape}, train targets {train_targets_tensor.shape}\")\n",
    "print(f\"Normalized t2m_f mean: {train_targets_tensor[:, :, 5].mean().item():.2f}, std: {train_targets_tensor[:, :, 5].std().item():.2f}\")\n",
    "\n",
    "# Temporary Train/Test split (90/10)\n",
    "train_size = int(0.9 * train_inputs_tensor.shape[0])\n",
    "print(f\"Train size: {train_size}\")\n",
    "temp_train_inputs = train_inputs_tensor[:train_size]\n",
    "temp_train_targets = train_targets_tensor[:train_size]\n",
    "temp_test_inputs = train_inputs_tensor[train_size:]\n",
    "temp_test_targets = train_targets_tensor[train_size:]\n",
    "print(f\"Train/test split: train shape {temp_train_inputs.shape}, test shape {temp_test_inputs.shape}\")\n",
    "\n",
    "# Limit training data to first 100 time steps\n",
    "temp_train_inputs = temp_train_inputs[:100]\n",
    "temp_train_targets = temp_train_targets[:100]\n",
    "print(f\"Limited training data to first 100 time steps: train shape {temp_train_inputs.shape}\")\n",
    "\n",
    "# Move tensors to GPU upfront\n",
    "temp_train_inputs = temp_train_inputs.to(device)\n",
    "temp_train_targets = temp_train_targets.to(device)\n",
    "temp_test_inputs = temp_test_inputs.to(device)\n",
    "temp_test_targets = temp_test_targets.to(device)\n",
    "\n",
    "# Prepare data for batching\n",
    "batch_size = 4  # Reduced batch size\n",
    "seq_len = 5     # Reduced sequence length\n",
    "num_nodes = 23937\n",
    "num_train_steps = temp_train_inputs.shape[0] - 1\n",
    "num_batches = (num_train_steps - seq_len) // batch_size\n",
    "num_test_steps = temp_test_inputs.shape[0] - 1\n",
    "val_steps = list(range(0, max(1, num_test_steps - seq_len + 1), 2))\n",
    "print(f\"Batching: batch_size={batch_size}, seq_len={seq_len}, num_batches={num_batches}, val_steps={val_steps}\")\n",
    "\n",
    "# Mixed precision training setup\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "# Define k values to test\n",
    "k_values = [4, 8, 24]\n",
    "results = {}  # Store MAE and RMSE for each k\n",
    "\n",
    "# Coordinates for edge index (computed once)\n",
    "lat_subset = np.linspace(50, 25, 101)\n",
    "lon_subset = np.linspace(235, 294, 237)\n",
    "coords = np.stack(np.meshgrid(lat_subset, lon_subset, indexing='ij'), axis=-1).reshape(-1, 2)\n",
    "\n",
    "# WeatherGNNGRU model\n",
    "class WeatherGNNGRU(t.nn.Module):\n",
    "    def __init__(self, num_features=15, hidden_dim=64, gru_hidden=16, num_outputs=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.gru = nn.GRU(hidden_dim, gru_hidden, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(gru_hidden, num_outputs)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.residual = nn.Linear(num_features, num_outputs)\n",
    "        self.res_weight = nn.Parameter(t.tensor(2.0))\n",
    "\n",
    "    def forward(self, x, edge_index, batch_size, seq_len, num_nodes):\n",
    "        batch_size, seq_len, num_nodes, num_features = x.shape\n",
    "        x = x.view(batch_size * seq_len, num_nodes, num_features)\n",
    "        residual = self.residual(x).view(batch_size, seq_len, num_nodes, 1) * self.res_weight\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = x.view(batch_size, seq_len, num_nodes, -1)\n",
    "        x = x.mean(dim=2)\n",
    "        x, _ = self.gru(x)\n",
    "        out = self.fc(x)\n",
    "        out = out[:, -1, :].unsqueeze(-1).expand(-1, num_nodes, -1)\n",
    "        residual = residual[:, -1, :, :]\n",
    "        return out + residual\n",
    "\n",
    "loss_fns = {\n",
    "    'L1': lambda x, y: t.mean(t.abs(x - y)),\n",
    "    'L2': lambda x, y: t.mean((x - y) ** 2),\n",
    "    'Huber': nn.SmoothL1Loss(reduction='mean')\n",
    "}\n",
    "\n",
    "# Pre-compute t2m_f statistics for denormalization\n",
    "t2m_f_mean = 42.36  # °F\n",
    "t2m_f_std = 21.75   # °F\n",
    "\n",
    "# Loop over k values\n",
    "for k in k_values:\n",
    "    print(f\"\\nTesting with k={k} neighbors...\")\n",
    "    \n",
    "    # Compute edge index for current k\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1).fit(coords)\n",
    "    _, indices = nbrs.kneighbors(coords)\n",
    "    edge_index = t.tensor(np.stack([np.repeat(np.arange(num_nodes), k), indices[:, 1:].flatten()]), dtype=t.long).to(device)\n",
    "    print(f\"Edge index computed for k={k}, shape: {edge_index.shape}\")\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    num_epochs = 10\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for loss_name, criterion in loss_fns.items():\n",
    "        print(f\"\\nTraining with {loss_name} loss for t2m_f prediction (k={k})...\")\n",
    "        t.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        model = WeatherGNNGRU(num_features=15, hidden_dim=64, gru_hidden=16, num_outputs=1).to(device)\n",
    "        optimizer = t.optim.Adam(model.parameters(), lr=0.01)\n",
    "        scheduler = t.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
    "        train_losses, val_losses = [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start = time.time()\n",
    "            print(f\"Starting Epoch {epoch+1}/{num_epochs}\")\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            num_valid_batches = 0\n",
    "\n",
    "            for batch_idx in range(num_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "                if end_idx + seq_len > num_train_steps:\n",
    "                    continue\n",
    "                input_x = temp_train_inputs[start_idx:end_idx + seq_len].unfold(0, seq_len, 1).permute(0, 1, 3, 2)\n",
    "                target_y = temp_train_targets[start_idx:end_idx + seq_len, :, 5].unfold(0, seq_len, 1).unsqueeze(-1)\n",
    "                input_x = input_x.reshape(-1, seq_len, num_nodes, 15)\n",
    "                target_y = target_y.reshape(-1, seq_len, num_nodes, 1)\n",
    "                optimizer.zero_grad()\n",
    "                with autocast('cuda'):\n",
    "                    out = model(input_x, edge_index, input_x.shape[0], seq_len, num_nodes)\n",
    "                    loss = criterion(out, target_y[:, -1, :, :])\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "                num_valid_batches += 1\n",
    "\n",
    "            scheduler.step()\n",
    "            avg_loss = total_loss / max(num_valid_batches, 1)\n",
    "            train_losses.append(avg_loss)\n",
    "            epoch_time = time.time() - epoch_start\n",
    "            print(f\"Finished Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}, Total runtime: {epoch_time:.2f}s\")\n",
    "\n",
    "            if epoch % 2 == 0 or epoch == num_epochs - 1:\n",
    "                val_start = time.time()\n",
    "                print(f\"Starting Validation for Epoch {epoch+1}\")\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                num_valid_val_batches = 0\n",
    "                with t.no_grad():\n",
    "                    for t_step in val_steps:\n",
    "                        if t_step + seq_len > num_test_steps:\n",
    "                            continue\n",
    "                        input_x = temp_test_inputs[t_step:t_step + seq_len].unsqueeze(0)\n",
    "                        target_y = temp_test_targets[t_step:t_step + seq_len, :, 5].unsqueeze(0).unsqueeze(-1)\n",
    "                        with autocast('cuda'):\n",
    "                            out = model(input_x, edge_index, 1, seq_len, num_nodes)\n",
    "                            val_loss += criterion(out, target_y[:, -1, :, :]).item()\n",
    "                        num_valid_val_batches += 1\n",
    "                val_loss = val_loss / max(num_valid_val_batches, 1)\n",
    "                val_losses.append(val_loss)\n",
    "                val_time = time.time() - val_start\n",
    "                print(f\"Finished Validation for Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Total runtime: {val_time:.2f}s\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    best_model_state = model.state_dict()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}: Validation loss has not improved for {patience} validations.\")\n",
    "                        break\n",
    "\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "            print(\"Loaded best model state based on validation loss.\")\n",
    "\n",
    "        # Test evaluation\n",
    "        test_start = time.time()\n",
    "        print(f\"Starting Final Test Evaluation for {loss_name} loss (k={k})\")\n",
    "        model.eval()\n",
    "        test_preds, test_trues = [], []\n",
    "        with t.no_grad():\n",
    "            for t_step in range(num_test_steps - seq_len + 1):\n",
    "                input_x = temp_test_inputs[t_step:t_step + seq_len].unsqueeze(0)\n",
    "                target_y = temp_test_targets[t_step:t_step + seq_len, :, 5].unsqueeze(0).unsqueeze(-1)\n",
    "                with autocast('cuda'):\n",
    "                    out = model(input_x, edge_index, 1, seq_len, num_nodes)\n",
    "                test_preds.append(out[:, :, 0].cpu())\n",
    "                test_trues.append(target_y[:, -1, :, 0].cpu())\n",
    "        test_preds = t.cat(test_preds, dim=0)\n",
    "        test_trues = t.cat(test_trues, dim=0)\n",
    "        test_time = time.time() - test_start\n",
    "        print(f\"Finished Final Test Evaluation for {loss_name} loss, Total runtime: {test_time:.2f}s\")\n",
    "\n",
    "        # Denormalize and evaluate t2m_f\n",
    "        preds_t2m = test_preds * t2m_f_std + t2m_f_mean\n",
    "        trues_t2m = test_trues * t2m_f_std + t2m_f_mean\n",
    "        mae_t2m = t.mean(t.abs(preds_t2m - trues_t2m)).item()\n",
    "        rmse_t2m = t.sqrt(t.mean((preds_t2m - trues_t2m) ** 2)).item()\n",
    "        print(f\"t2m_f L1 norm (°F) (k={k}): {mae_t2m:.2f}\")\n",
    "        print(f\"t2m_f RMSE (°F) (k={k}): {rmse_t2m:.2f}\")\n",
    "\n",
    "        # Store results\n",
    "        results[(k, loss_name)] = {'MAE': mae_t2m, 'RMSE': rmse_t2m}\n",
    "\n",
    "        # Denormalize sample outputs\n",
    "        sample_preds = preds_t2m[:5, 0].numpy()\n",
    "        sample_targets = trues_t2m[:5, 0].numpy()\n",
    "        print(f\"Sample t2m_f preds (°F) (k={k}): {sample_preds}\")\n",
    "        print(f\"Sample t2m_f targets (°F) (k={k}): {sample_targets}\")\n",
    "\n",
    "        # Loss Plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, len(train_losses) + 1), train_losses, label=f'Train Loss ({loss_name})', color='blue')\n",
    "        plt.plot([i * 2 for i in range(len(val_losses))], val_losses, label='Validation Loss', color='orange', marker='o')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(f'{loss_name} Loss (Normalized)')\n",
    "        plt.title(f'Training and Validation Loss for t2m_f ({loss_name}, k={k})')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'loss_plot_t2m_f_{loss_name}_k{k}.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Time Series Plot for t2m_f\n",
    "        plot_start = time.time()\n",
    "        print(f\"Starting Time Series Plotting for t2m_f (k={k})\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(preds_t2m[:, 0].numpy(), label='Pred t2m_f', color='red', linestyle='--')\n",
    "        plt.plot(trues_t2m[:, 0].numpy(), label='True t2m_f', color='green')\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Temperature (°F)')\n",
    "        plt.title(f'Predicted vs Actual t2m_f (k={k})')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f't2m_f_timeseries_k{k}.png')\n",
    "        print(f\"Finished Time Series Plotting, Total runtime: {time.time() - plot_start:.2f}s\")\n",
    "        plt.show()\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\nSummary of Results:\")\n",
    "for (k, loss_name), metrics in results.items():\n",
    "    print(f\"k={k}, Loss={loss_name}: MAE={metrics['MAE']:.2f}°F, RMSE={metrics['RMSE']:.2f}°F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e19d2d",
   "metadata": {},
   "source": [
    "# Fourth Iteration: Replacing the GRU Layer with a Temporal Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139ea775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import gc\n",
    "import os\n",
    "\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tensors (assuming they are pre-normalized and stored)\n",
    "train_inputs_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_inputs_norm.pt').to(device)\n",
    "train_targets_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_targets_norm.pt').to(device)\n",
    "print(f\"Tensors loaded: train inputs {train_inputs_tensor.shape}, train targets {train_targets_tensor.shape}\")\n",
    "\n",
    "# Temporary Train/Test split (90% train, 10% test)\n",
    "train_size = int(0.9 * train_inputs_tensor.shape[0])\n",
    "temp_train_inputs = train_inputs_tensor[:100]\n",
    "temp_train_targets = train_targets_tensor[:100]\n",
    "temp_test_inputs = train_inputs_tensor[train_size:]\n",
    "temp_test_targets = train_targets_tensor[train_size:]\n",
    "print(f\"Train/test split: train shape {temp_train_inputs.shape}, test shape {temp_test_inputs.shape}\")\n",
    "\n",
    "# Move tensors to GPU\n",
    "temp_train_inputs = temp_train_inputs.to(device)\n",
    "temp_train_targets = temp_train_targets.to(device)\n",
    "temp_test_inputs = temp_test_inputs.to(device)\n",
    "temp_test_targets = temp_test_targets.to(device)\n",
    "\n",
    "# Define directories for saving plots and models\n",
    "loss_plot_dir = r'F:/weather_forecasting/notebooks/final project/png/model construction & training/1dConv/loss plots'\n",
    "pred_plot_dir = r'F:/weather_forecasting/notebooks/final project/png/model construction & training/1dConv/pred vs true'\n",
    "model_save_dir = r'F:/weather_forecasting/notebooks/final project/models/1dConv'\n",
    "os.makedirs(loss_plot_dir, exist_ok=True)\n",
    "os.makedirs(pred_plot_dir, exist_ok=True)\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Function to create windowed batches on-the-fly (memory-efficient)\n",
    "def windowed_batch_generator(inputs_tensor, targets_tensor, window_size, batch_size, num_nodes, device):\n",
    "    num_time_steps = inputs_tensor.shape[0]\n",
    "    for start in range(window_size - 1, num_time_steps - 1, batch_size):\n",
    "        end = min(start + batch_size, num_time_steps - 1)\n",
    "        windows = []\n",
    "        target_batch = []\n",
    "        for i in range(start, end):\n",
    "            window = inputs_tensor[i - window_size + 1:i + 1].squeeze(-1).to(device)  # Shape: [window_size, num_nodes, num_features]\n",
    "            windows.append(window)\n",
    "            target = targets_tensor[i + 1, :, 5, :].squeeze(-1).to(device)  # Shape: [num_nodes]\n",
    "            target_batch.append(target)\n",
    "        yield t.stack(windows), t.stack(target_batch).unsqueeze(-1)  # [batch_size, window_size, num_nodes, num_features], [batch_size, num_nodes, 1]\n",
    "\n",
    "class WeatherGNNWindow(nn.Module):\n",
    "    def __init__(self, num_features=15, window_size=1, new_features=64, hidden_dim=128, num_outputs=1):\n",
    "        super().__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.temporal = nn.Conv1d(\n",
    "            in_channels=num_features,\n",
    "            out_channels=new_features,\n",
    "            kernel_size=window_size\n",
    "        )\n",
    "        self.gcn1 = GCNConv(new_features, hidden_dim)\n",
    "        self.gcn2 = GCNConv(hidden_dim, num_outputs)\n",
    "        self.residual = nn.Linear(new_features, num_outputs)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x, edge_index, batch_size, num_nodes):\n",
    "        batch_size, window_size, num_nodes, num_features = x.shape\n",
    "        x = x.permute(0, 2, 3, 1)  # [batch_size, num_nodes, num_features, window_size]\n",
    "        x = x.reshape(batch_size * num_nodes, num_features, window_size)\n",
    "        temporal_out = self.temporal(x).squeeze(-1)  # [batch_size * num_nodes, new_features]\n",
    "        residual = self.residual(temporal_out)      # [batch_size * num_nodes, num_outputs]\n",
    "        edge_index_list = [edge_index + i * num_nodes for i in range(batch_size)]\n",
    "        batched_edge_index = t.cat(edge_index_list, dim=1)\n",
    "        x = self.gcn1(temporal_out, batched_edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.gcn2(x, batched_edge_index)\n",
    "        out = x + residual\n",
    "        out = out.view(batch_size, num_nodes, self.num_outputs)\n",
    "        return out\n",
    "\n",
    "# Define parameters\n",
    "num_nodes = 23937\n",
    "lat_subset = np.linspace(50, 25, 101)\n",
    "lon_subset = np.linspace(235, 294, 237)\n",
    "coords = np.stack(np.meshgrid(lat_subset, lon_subset, indexing='ij'), axis=-1).reshape(-1, 2)\n",
    "window_sizes = [1, 3, 6, 12]\n",
    "k_values = [4, 8, 24]\n",
    "loss_fns = {\n",
    "    'L1': lambda x, y: t.mean(t.abs(x - y)),\n",
    "    'L2': lambda x, y: t.mean((x - y) ** 2),\n",
    "    'Huber': nn.SmoothL1Loss(reduction='mean')\n",
    "}\n",
    "t2m_f_mean = 42.36  # °F\n",
    "t2m_f_std = 21.75   # °F\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Main loop over window sizes\n",
    "for window_size in window_sizes:\n",
    "    print(f\"\\n--- Experiment with window_size={window_size} ---\")\n",
    "    loss_curves = {}\n",
    "    pred_data = {}\n",
    "    \n",
    "    # Adjust target tensors\n",
    "    train_targets = temp_train_targets[window_size:]\n",
    "    test_targets = temp_test_targets[window_size:]\n",
    "    \n",
    "    for k in k_values:\n",
    "        print(f\"\\nTesting with k={k} neighbors...\")\n",
    "        # Compute edge index for current k\n",
    "        nbrs = NearestNeighbors(n_neighbors=k+1).fit(coords)\n",
    "        _, indices = nbrs.kneighbors(coords)\n",
    "        edge_index = t.tensor(np.stack([np.repeat(np.arange(num_nodes), k), indices[:, 1:].flatten()]), dtype=t.long).to(device)\n",
    "        \n",
    "        for loss_name, criterion in loss_fns.items():\n",
    "            print(f\"\\nTraining with {loss_name} loss (k={k})...\")\n",
    "            t.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            model = WeatherGNNWindow(num_features=15, window_size=window_size, num_outputs=1).to(device)\n",
    "            optimizer = t.optim.Adam(model.parameters(), lr=0.01)\n",
    "            scheduler = t.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
    "            train_losses, val_losses = [], []\n",
    "            val_epochs = []\n",
    "\n",
    "            # Training loop\n",
    "            for epoch in range(num_epochs):\n",
    "                epoch_start = time.time()\n",
    "                model.train()\n",
    "                total_loss = 0\n",
    "                batch_count = 0\n",
    "                for batch_inputs, batch_targets in windowed_batch_generator(temp_train_inputs, temp_train_targets, window_size, batch_size, num_nodes, device):\n",
    "                    optimizer.zero_grad()\n",
    "                    out = model(batch_inputs, edge_index, batch_size=batch_inputs.shape[0], num_nodes=num_nodes)\n",
    "                    loss = criterion(out, batch_targets)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                    batch_count += 1\n",
    "                avg_loss = total_loss / batch_count\n",
    "                train_losses.append(avg_loss)\n",
    "                epoch_time = time.time() - epoch_start\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "                if epoch % 2 == 0 or epoch == num_epochs - 1:\n",
    "                    val_start = time.time()\n",
    "                    model.eval()\n",
    "                    val_loss = 0\n",
    "                    val_batch_count = 0\n",
    "                    with t.no_grad():\n",
    "                        for batch_inputs, batch_targets in windowed_batch_generator(temp_test_inputs, temp_test_targets, window_size, batch_size, num_nodes, device):\n",
    "                            out = model(batch_inputs, edge_index, batch_size=batch_inputs.shape[0], num_nodes=num_nodes)\n",
    "                            val_loss += criterion(out, batch_targets).item()\n",
    "                            val_batch_count += 1\n",
    "                    val_loss /= val_batch_count\n",
    "                    val_losses.append(val_loss)\n",
    "                    val_epochs.append(epoch + 1)\n",
    "                    val_time = time.time() - val_start\n",
    "                    print(f\"Validation Loss: {val_loss:.4f}, Validation Time: {val_time:.2f}s\")\n",
    "\n",
    "            # Save the trained model\n",
    "            model_path = os.path.join(model_save_dir, f'weather_gnn_window{window_size}_k{k}_{loss_name}.pth')\n",
    "            t.save(model.state_dict(), model_path)\n",
    "            print(f\"Saved model to {model_path}\")\n",
    "\n",
    "            # Test evaluation\n",
    "            model.eval()\n",
    "            test_preds, test_trues = [], []\n",
    "            with t.no_grad():\n",
    "                for batch_inputs, batch_targets in windowed_batch_generator(temp_test_inputs, temp_test_targets, window_size, batch_size, num_nodes, device):\n",
    "                    out = model(batch_inputs, edge_index, batch_size=batch_inputs.shape[0], num_nodes=num_nodes)\n",
    "                    test_preds.append(out.cpu())\n",
    "                    test_trues.append(batch_targets.cpu())\n",
    "            preds_t2m = t.cat(test_preds, dim=0) * t2m_f_std + t2m_f_mean\n",
    "            trues_t2m = t.cat(test_trues, dim=0) * t2m_f_std + t2m_f_mean\n",
    "\n",
    "            # Save individual loss plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', color='blue')\n",
    "            plt.plot(val_epochs, val_losses, label='Validation Loss', color='orange', marker='o')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel(f'{loss_name} Loss (Normalized)')\n",
    "            plt.title(f'Loss Curves (window_size={window_size}, k={k}, {loss_name})')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(loss_plot_dir, f'loss_plot_window{window_size}_k{k}_{loss_name}.png'))\n",
    "            plt.close()\n",
    "\n",
    "            # Save individual pred vs true plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(preds_t2m[:, 0, 0].numpy(), label='Pred t2m_f', color='red', linestyle='--')\n",
    "            plt.plot(trues_t2m[:, 0, 0].numpy(), label='True t2m_f', color='green')\n",
    "            plt.xlabel('Time Step')\n",
    "            plt.ylabel('Temperature (°F)')\n",
    "            plt.title(f'Pred vs True t2m_f (window_size={window_size}, k={k}, {loss_name})')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(pred_plot_dir, f't2m_f_timeseries_window{window_size}_k{k}_{loss_name}.png'))\n",
    "            plt.close()\n",
    "\n",
    "            # Store data for combined plots\n",
    "            loss_curves[(k, loss_name)] = {'train': train_losses, 'val': val_losses, 'val_epochs': val_epochs}\n",
    "            pred_data[(k, loss_name)] = {'preds': preds_t2m[:, 0, 0].numpy(), 'trues': trues_t2m[:, 0, 0].numpy()}\n",
    "\n",
    "    # Combined loss curves\n",
    "    fig, axes = plt.subplots(len(k_values), len(loss_fns), figsize=(15, 10), sharex=True, sharey=True)\n",
    "    for i, k in enumerate(k_values):\n",
    "        for j, loss_name in enumerate(loss_fns):\n",
    "            ax = axes[i, j]\n",
    "            data = loss_curves.get((k, loss_name), None)\n",
    "            if data:\n",
    "                ax.plot(range(1, len(data['train']) + 1), data['train'], label='Train')\n",
    "                ax.plot(data['val_epochs'], data['val'], label='Val', marker='o')\n",
    "                ax.set_title(f'k={k}, {loss_name}')\n",
    "                ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(loss_plot_dir, f'combined_loss_window{window_size}.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # Combined pred vs true\n",
    "    fig, axes = plt.subplots(len(k_values), len(loss_fns), figsize=(15, 10), sharex=True, sharey=True)\n",
    "    for i, k in enumerate(k_values):\n",
    "        for j, loss_name in enumerate(loss_fns):\n",
    "            ax = axes[i, j]\n",
    "            data = pred_data.get((k, loss_name), None)\n",
    "            if data:\n",
    "                num_steps = min(100, len(data['preds']))\n",
    "                ax.plot(data['preds'][:num_steps], label='Pred', linestyle='--')\n",
    "                ax.plot(data['trues'][:num_steps], label='True')\n",
    "                ax.set_title(f'k={k}, {loss_name}')\n",
    "                ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(pred_plot_dir, f'combined_pred_vs_true_window{window_size}.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867bdff2",
   "metadata": {},
   "source": [
    "# Chosen Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36740329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import gc\n",
    "import os\n",
    "\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tensors\n",
    "train_inputs_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_inputs_norm.pt').to(device)\n",
    "train_targets_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_targets_norm.pt').to(device)\n",
    "print(f\"Tensors loaded: train inputs {train_inputs_tensor.shape}, train targets {train_targets_tensor.shape}\")\n",
    "print(f\"Normalized t2m_f mean: {train_targets_tensor[:, :, 5, :].mean().item():.2f}, std: {train_targets_tensor[:, :, 5, :].std().item():.2f}\")\n",
    "\n",
    "# Temporary Train/Test split\n",
    "train_size = int(0.9 * train_inputs_tensor.shape[0])\n",
    "print(f\"Train size: {train_size}\")\n",
    "temp_train_inputs = train_inputs_tensor[:train_size]\n",
    "temp_train_targets = train_targets_tensor[:train_size]\n",
    "temp_test_inputs = train_inputs_tensor[train_size:]\n",
    "temp_test_targets = train_targets_tensor[train_size:]\n",
    "print(f\"Train/test split: train shape {temp_train_inputs.shape}, test shape {temp_test_inputs.shape}\")\n",
    "\n",
    "# Move tensors to GPU upfront\n",
    "temp_train_inputs = temp_train_inputs.to(device)\n",
    "temp_train_targets = temp_train_targets.to(device)\n",
    "temp_test_inputs = temp_test_inputs.to(device)\n",
    "temp_test_targets = temp_test_targets.to(device)\n",
    "\n",
    "# Edge index (computed on CPU, then moved to GPU)\n",
    "num_nodes = 23937\n",
    "k = 8\n",
    "lat_subset = np.linspace(50, 25, 101)\n",
    "lon_subset = np.linspace(235, 294, 237)\n",
    "coords = np.stack(np.meshgrid(lat_subset, lon_subset, indexing='ij'), axis=-1).reshape(-1, 2)\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1).fit(coords)\n",
    "_, indices = nbrs.kneighbors(coords)\n",
    "edge_index = t.tensor(np.stack([np.repeat(np.arange(num_nodes), k), indices[:, 1:].flatten()]), dtype=t.long).to(device)\n",
    "\n",
    "# Model (simplified GNN without temporal layer)\n",
    "class WeatherGNN(t.nn.Module):\n",
    "    def __init__(self, num_features=15, hidden_dims=128, num_outputs=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dims)\n",
    "        self.conv2 = GCNConv(hidden_dims, hidden_dims)\n",
    "        self.conv3 = GCNConv(hidden_dims, num_outputs)\n",
    "        self.dropout = t.nn.Dropout(0.3)\n",
    "        self.residual = t.nn.Linear(num_features, num_outputs)\n",
    "        self.res_weight = t.nn.Parameter(t.tensor(2.0))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        residual = self.residual(x) * self.res_weight\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x + residual\n",
    "\n",
    "# Define L1 loss function directly\n",
    "def l1_loss(x, y):\n",
    "    return t.mean(t.abs(x - y))\n",
    "\n",
    "# Pre-compute t2m_f statistics for denormalization\n",
    "t2m_f_std = train_targets_tensor[:, :, 5, :].std().item()\n",
    "t2m_f_mean = train_targets_tensor[:, :, 5, :].mean().item()\n",
    "t2m_f_mean = 42.36  # °F\n",
    "t2m_f_std = 21.75   # °F\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 10\n",
    "val_steps = list(range(0, temp_test_inputs.shape[0] - 1, 2))\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(\"\\nTraining with L1 loss for t2m_f prediction...\")\n",
    "t.cuda.empty_cache()\n",
    "gc.collect()\n",
    "model = WeatherGNN(num_features=15, hidden_dims=128, num_outputs=1).to(device)\n",
    "optimizer = t.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = t.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
    "train_losses, val_losses = [], []\n",
    "val_epochs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"Starting Epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for t_step in range(temp_train_inputs.shape[0] - 1):\n",
    "        input_x = temp_train_inputs[t_step].reshape(num_nodes, -1)\n",
    "        target_y = temp_train_targets[t_step, :, 5, :].reshape(num_nodes, -1)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(input_x, edge_index)\n",
    "        loss = l1_loss(out, target_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / (temp_train_inputs.shape[0] - 1)\n",
    "    train_losses.append(avg_loss)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"Finished Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}, Total runtime: {epoch_time:.2f}s\")\n",
    "\n",
    "    if epoch % 2 == 0 or epoch == num_epochs - 1:\n",
    "        val_start = time.time()\n",
    "        print(f\"Starting Validation for Epoch {epoch+1}\")\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with t.no_grad():\n",
    "            for t_step in val_steps:\n",
    "                input_x = temp_test_inputs[t_step].reshape(num_nodes, -1)\n",
    "                target_y = temp_test_targets[t_step, :, 5, :].reshape(num_nodes, -1)\n",
    "                out = model(input_x, edge_index)\n",
    "                val_loss += l1_loss(out, target_y).item()\n",
    "        val_loss = val_loss / len(val_steps)\n",
    "        val_losses.append(val_loss)\n",
    "        val_time = time.time() - val_start\n",
    "        val_epochs.append(epoch + 1)\n",
    "        print(f\"Finished Validation for Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Total runtime: {val_time:.2f}s\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}: Validation loss has not improved for {patience} validations.\")\n",
    "                break\n",
    "\n",
    "# Save the best model state\n",
    "t.save(best_model_state, 'best_weather_gnn_model.pth')\n",
    "print(\"Best model saved to 'best_weather_gnn_model.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219f7e48",
   "metadata": {},
   "source": [
    "# Chosen Model: Different Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17832781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import gc\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tensors\n",
    "train_inputs_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_inputs_norm.pt').to(device)\n",
    "train_targets_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_targets_norm.pt').to(device)\n",
    "print(f\"Tensors loaded: train inputs {train_inputs_tensor.shape}, train targets {train_targets_tensor.shape}\")\n",
    "print(f\"Normalized t2m_f mean: {train_targets_tensor[:, :, 5, :].mean().item():.2f}, std: {train_targets_tensor[:, :, 5, :].std().item():.2f}\")\n",
    "\n",
    "# Temporary Train/Test split\n",
    "train_size = int(0.9 * train_inputs_tensor.shape[0])\n",
    "print(f\"Train size: {train_size}\")\n",
    "temp_train_inputs = train_inputs_tensor[:train_size]\n",
    "temp_train_targets = train_targets_tensor[:train_size]\n",
    "temp_test_inputs = train_inputs_tensor[train_size:]\n",
    "temp_test_targets = train_targets_tensor[train_size:]\n",
    "print(f\"Train/test split: train shape {temp_train_inputs.shape}, test shape {temp_test_inputs.shape}\")\n",
    "\n",
    "# Move tensors to GPU upfront\n",
    "temp_train_inputs = temp_train_inputs.to(device)\n",
    "temp_train_targets = temp_train_targets.to(device)\n",
    "temp_test_inputs = temp_test_inputs.to(device)\n",
    "temp_test_targets = temp_test_targets.to(device)\n",
    "\n",
    "# Edge index (computed on CPU, then moved to GPU)\n",
    "num_nodes = 23937\n",
    "k = 8\n",
    "lat_subset = np.linspace(50, 25, 101)\n",
    "lon_subset = np.linspace(235, 294, 237)\n",
    "coords = np.stack(np.meshgrid(lat_subset, lon_subset, indexing='ij'), axis=-1).reshape(-1, 2)\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1).fit(coords)\n",
    "_, indices = nbrs.kneighbors(coords)\n",
    "edge_index = t.tensor(np.stack([np.repeat(np.arange(num_nodes), k), indices[:, 1:].flatten()]), dtype=t.long).to(device)\n",
    "\n",
    "# Model (simplified GNN without temporal layer)\n",
    "class WeatherGNN(t.nn.Module):\n",
    "    def __init__(self, num_features=15, hidden_dims=128, num_outputs=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dims)\n",
    "        self.conv2 = GCNConv(hidden_dims, hidden_dims)\n",
    "        self.conv3 = GCNConv(hidden_dims, num_outputs)\n",
    "        self.dropout = t.nn.Dropout(0.3)\n",
    "        self.residual = t.nn.Linear(num_features, num_outputs)\n",
    "        self.res_weight = t.nn.Parameter(t.tensor(2.0))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        residual = self.residual(x) * self.res_weight\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x + residual\n",
    "\n",
    "# Define L1 loss function directly\n",
    "def l1_loss(x, y):\n",
    "    return t.mean(t.abs(x - y))\n",
    "\n",
    "# Pre-compute t2m_f statistics for denormalization\n",
    "t2m_f_std = train_targets_tensor[:, :, 5, :].std().item()\n",
    "t2m_f_mean = train_targets_tensor[:, :, 5, :].mean().item()\n",
    "t2m_f_mean = 42.36  # °F\n",
    "t2m_f_std = 21.75   # °F\n",
    "\n",
    "# Learning rate configurations\n",
    "learning_rates = [\n",
    "    (0.01, False),   # Stationary\n",
    "    (0.005, False),  # Stationary\n",
    "    (0.001, False),  # Stationary\n",
    "    (0.0005, False), # Stationary\n",
    "    (0.01, True),    # Adaptive with StepLR\n",
    "    (0.005, True),   # Adaptive with StepLR\n",
    "    (0.001, True),   # Adaptive with StepLR\n",
    "    (0.0005, True)   # Adaptive with StepLR\n",
    "]\n",
    "\n",
    "# Store results for plotting\n",
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "all_val_epochs = []\n",
    "labels = []\n",
    "\n",
    "# Training loop for each learning rate\n",
    "num_epochs = 18\n",
    "val_steps = list(range(0, temp_test_inputs.shape[0] - 1, 2))\n",
    "patience = 5\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "for lr, use_adaptive in learning_rates:\n",
    "    # Generate unique model filename\n",
    "    lr_str = str(lr).replace('.', 'p')\n",
    "    scheduler_str = 'adaptive' if use_adaptive else 'stationary'\n",
    "    model_filename = f'model_lr{lr_str}_{scheduler_str}_{timestamp}.pth'\n",
    "    \n",
    "    print(f\"\\nTraining with learning rate {lr}, Scheduler: {scheduler_str}...\")\n",
    "    t.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model = WeatherGNN(num_features=15, num_outputs=1).to(device)\n",
    "    optimizer = t.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = t.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.9) if use_adaptive else None\n",
    "    train_losses, val_losses = [], []\n",
    "    val_epochs = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(f\"Starting Epoch {epoch+1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for t_step in range(temp_train_inputs.shape[0] - 1):\n",
    "            input_x = temp_train_inputs[t_step].reshape(num_nodes, -1)\n",
    "            target_y = temp_train_targets[t_step, :, 5, :].reshape(num_nodes, -1)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(input_x, edge_index)\n",
    "            loss = l1_loss(out, target_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        avg_loss = total_loss / (temp_train_inputs.shape[0] - 1)\n",
    "        train_losses.append(avg_loss)\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Finished Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}, Total runtime: {epoch_time:.2f}s\")\n",
    "\n",
    "        if epoch % 2 == 0 or epoch == num_epochs - 1:\n",
    "            val_start = time.time()\n",
    "            print(f\"Starting Validation for Epoch {epoch+1}\")\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with t.no_grad():\n",
    "                for t_step in val_steps:\n",
    "                    input_x = temp_test_inputs[t_step].reshape(num_nodes, -1)\n",
    "                    target_y = temp_test_targets[t_step, :, 5, :].reshape(num_nodes, -1)\n",
    "                    out = model(input_x, edge_index)\n",
    "                    val_loss += l1_loss(out, target_y).item()\n",
    "            val_loss = val_loss / len(val_steps)\n",
    "            val_losses.append(val_loss)\n",
    "            val_epochs.append(epoch + 1)\n",
    "            val_time = time.time() - val_start\n",
    "            print(f\"Finished Validation for Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Total runtime: {val_time:.2f}s\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_model_state = model.state_dict()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}: Validation loss has not improved for {patience} validations.\")\n",
    "                    break\n",
    "\n",
    "    # Save the best model state\n",
    "    t.save(best_model_state, model_filename)\n",
    "    print(f\"Best model saved to '{model_filename}'\")\n",
    "\n",
    "    # Create executable with PyInstaller\n",
    "    print(f\"Starting creation of executable for {model_filename}\")\n",
    "    exe_start = time.time()\n",
    "    # Create a temporary script for the model\n",
    "    script_content = f\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class WeatherGNN(nn.Module):\n",
    "    def __init__(self, num_features=15, num_outputs=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, 128)\n",
    "        self.conv2 = GCNConv(128, 128)\n",
    "        self.conv3 = GCNConv(128, num_outputs)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.residual = nn.Linear(num_features, num_outputs)\n",
    "        self.res_weight = nn.Parameter(torch.tensor(2.0))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        residual = self.residual(x) * self.res_weight\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x + residual\n",
    "\n",
    "def load_model(model_path):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = WeatherGNN(num_features=15, num_outputs=1).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = load_model('{model_filename}')\n",
    "    print(f\"Loaded model from {model_filename}\")\n",
    "\"\"\"\n",
    "    temp_script_path = f'temp_model_script_{lr_str}_{scheduler_str}_{timestamp}.py'\n",
    "    with open(temp_script_path, 'w') as f:\n",
    "        f.write(script_content)\n",
    "\n",
    "    # Run PyInstaller to create executable\n",
    "    exe_filename = f'model_lr{lr_str}_{scheduler_str}_{timestamp}'\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            'pyinstaller',\n",
    "            '--onefile',\n",
    "            '--name', exe_filename,\n",
    "            temp_script_path\n",
    "        ], check=True)\n",
    "        print(f\"Executable creation completed: {exe_filename}.exe in dist directory\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to create executable: {e}\")\n",
    "    finally:\n",
    "        # Clean up temporary script\n",
    "        if os.path.exists(temp_script_path):\n",
    "            os.remove(temp_script_path)\n",
    "    exe_time = time.time() - exe_start\n",
    "    print(f\"Executable creation took {exe_time:.2f} seconds\")\n",
    "\n",
    "    # Store results for plotting\n",
    "    all_train_losses.append(train_losses)\n",
    "    all_val_losses.append(val_losses)\n",
    "    all_val_epochs.append(val_epochs)\n",
    "    labels.append(f'lr={lr}, {scheduler_str}')\n",
    "\n",
    "# Plot loss curves in subplots\n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 20))\n",
    "axes = axes.flatten()\n",
    "for i, (train_losses, val_losses, val_epochs, label) in enumerate(zip(all_train_losses, all_val_losses, all_val_epochs, labels)):\n",
    "    ax = axes[i]\n",
    "    ax.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', color='blue')\n",
    "    ax.plot(val_epochs, val_losses, label='Validation Loss', color='orange', marker='o')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('L1 Loss (Normalized)')\n",
    "    ax.set_title(f'Loss Curves ({label})')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "fig.tight_layout()\n",
    "plt.savefig(f'loss_curves_comparison_{timestamp}.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot training vs validation loss in subplots\n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 20))\n",
    "axes = axes.flatten()\n",
    "for i, (train_losses, val_losses, val_epochs, label) in enumerate(zip(all_train_losses, all_val_losses, all_val_epochs, labels)):\n",
    "    ax = axes[i]\n",
    "    # Interpolate validation losses to match training loss length\n",
    "    val_losses_interp = np.interp(range(1, len(train_losses) + 1), val_epochs, val_losses)\n",
    "    ax.scatter(train_losses, val_losses_interp, marker='o')\n",
    "    ax.set_xlabel('Training Loss')\n",
    "    ax.set_ylabel('Validation Loss')\n",
    "    ax.set_title(f'Training vs Validation Loss ({label})')\n",
    "    ax.grid(True)\n",
    "fig.tight_layout()\n",
    "plt.savefig(f'train_vs_val_loss_comparison_{timestamp}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eed29f",
   "metadata": {},
   "source": [
    "# Chosen Model: Extended Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10ddf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import gc\n",
    "import os\n",
    "\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tensors\n",
    "train_inputs_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_inputs_norm.pt').to(device)\n",
    "train_targets_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_targets_norm.pt').to(device)\n",
    "print(f\"Tensors loaded: train inputs {train_inputs_tensor.shape}, train targets {train_targets_tensor.shape}\")\n",
    "print(f\"Normalized t2m_f mean: {train_targets_tensor[:, :, 5, :].mean().item():.2f}, std: {train_targets_tensor[:, :, 5, :].std().item():.2f}\")\n",
    "\n",
    "# Temporary Train/Test split\n",
    "train_size = int(0.9 * train_inputs_tensor.shape[0])\n",
    "print(f\"Train size: {train_size}\")\n",
    "temp_train_inputs = train_inputs_tensor[:train_size]\n",
    "temp_train_targets = train_targets_tensor[:train_size]\n",
    "temp_test_inputs = train_inputs_tensor[train_size:]\n",
    "temp_test_targets = train_targets_tensor[train_size:]\n",
    "print(f\"Train/test split: train shape {temp_train_inputs.shape}, test shape {temp_test_inputs.shape}\")\n",
    "\n",
    "# Move tensors to GPU upfront\n",
    "temp_train_inputs = temp_train_inputs.to(device)\n",
    "temp_train_targets = temp_train_targets.to(device)\n",
    "temp_test_inputs = temp_test_inputs.to(device)\n",
    "temp_test_targets = temp_test_targets.to(device)\n",
    "\n",
    "# Edge index (computed on CPU, then moved to GPU)\n",
    "num_nodes = 23937\n",
    "k = 8\n",
    "lat_subset = np.linspace(50, 25, 101)\n",
    "lon_subset = np.linspace(235, 294, 237)\n",
    "coords = np.stack(np.meshgrid(lat_subset, lon_subset, indexing='ij'), axis=-1).reshape(-1, 2)\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1).fit(coords)\n",
    "_, indices = nbrs.kneighbors(coords)\n",
    "edge_index = t.tensor(np.stack([np.repeat(np.arange(num_nodes), k), indices[:, 1:].flatten()]), dtype=t.long).to(device)\n",
    "\n",
    "# Model (updated with third GCNConv layer, lower hidden dimensions)\n",
    "class WeatherGNN(t.nn.Module):\n",
    "    def __init__(self, num_features=15, num_outputs=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, 128)  # Lowered hidden_dim\n",
    "        self.conv2 = GCNConv(128, 128)\n",
    "        self.conv3 = GCNConv(128, num_outputs)  # Added third layer\n",
    "        self.dropout = t.nn.Dropout(0.3)  # Increased dropout\n",
    "        self.residual = t.nn.Linear(num_features, num_outputs)\n",
    "        self.res_weight = t.nn.Parameter(t.tensor(2.0))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        residual = self.residual(x) * self.res_weight\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x + residual\n",
    "\n",
    "# Loss function (L1 only for now)\n",
    "loss_fns = {\n",
    "    'L1': lambda x, y: t.mean(t.abs(x - y))\n",
    "}\n",
    "\n",
    "# Pre-compute t2m_f statistics for denormalization (index 5)\n",
    "t2m_f_std = train_targets_tensor[:, :, 5, :].std().item()\n",
    "t2m_f_mean = train_targets_tensor[:, :, 5, :].mean().item()\n",
    "# Use previously validated stats for accuracy\n",
    "t2m_f_mean = 42.36  # °F\n",
    "t2m_f_std = 21.75   # °F\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 35  # Reduced epochs\n",
    "val_steps = list(range(0, temp_test_inputs.shape[0] - 1, 2))  # Validate every 2 steps\n",
    "patience = 5  # Early stopping patience\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for loss_name, criterion in loss_fns.items():\n",
    "    print(f\"\\nTraining with {loss_name} loss for t2m_f prediction...\")\n",
    "    t.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model = WeatherGNN(num_features=15, num_outputs=1).to(device)\n",
    "    optimizer = t.optim.Adam(model.parameters(), lr=0.01)\n",
    "    scheduler = t.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)  # Slower decay\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(f\"Starting Epoch {epoch+1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for t_step in range(temp_train_inputs.shape[0] - 1):\n",
    "            input_x = temp_train_inputs[t_step].reshape(num_nodes, -1)  # (23937, 15)\n",
    "            target_y = temp_train_targets[t_step, :, 5, :].reshape(num_nodes, -1)  # (23937, 1) for t2m_f\n",
    "            optimizer.zero_grad()\n",
    "            out = model(input_x, edge_index)\n",
    "            loss = criterion(out, target_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / (temp_train_inputs.shape[0] - 1)\n",
    "        train_losses.append(avg_loss)\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Finished Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}, Total runtime: {epoch_time:.2f}s\")\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "            val_start = time.time()\n",
    "            print(f\"Starting Validation for Epoch {epoch+1}\")\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with t.no_grad():\n",
    "                for t_step in val_steps:\n",
    "                    input_x = temp_test_inputs[t_step].reshape(num_nodes, -1)\n",
    "                    target_y = temp_test_targets[t_step, :, 5, :].reshape(num_nodes, -1)\n",
    "                    out = model(input_x, edge_index)\n",
    "                    val_loss += criterion(out, target_y).item()\n",
    "            val_loss = val_loss / len(val_steps)\n",
    "            val_losses.append(val_loss)\n",
    "            val_time = time.time() - val_start\n",
    "            print(f\"Finished Validation for Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Total runtime: {val_time:.2f}s\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_model_state = model.state_dict()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}: Validation loss has not improved for {patience} validations.\")\n",
    "                    break\n",
    "\n",
    "    # Save the best model state\n",
    "    t.save(best_model_state, r'f:\\weather_forecasting\\notebooks\\final project\\models\\paths\\best_weather_gnn_model_ext_training.pth')\n",
    "    print(\"Best model saved to 'f:/weather_forecasting/notebooks/final project/models/paths/best_weather_gnn_model_ext_training.pth'\")\n",
    "\n",
    "    # Load best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Loaded best model state based on validation loss.\")\n",
    "\n",
    "    # Test evaluation\n",
    "    test_start = time.time()\n",
    "    print(f\"Starting Final Test Evaluation for {loss_name} loss\")\n",
    "    model.eval()\n",
    "    test_preds, test_trues = [], []\n",
    "    with t.no_grad():\n",
    "        for t_step in range(temp_test_inputs.shape[0] - 1):\n",
    "            input_x = temp_test_inputs[t_step].reshape(num_nodes, -1)\n",
    "            target_y = temp_test_targets[t_step, :, 5, :].reshape(num_nodes, -1)\n",
    "            out = model(input_x, edge_index)\n",
    "            test_preds.append(out.cpu())\n",
    "            test_trues.append(target_y.cpu())\n",
    "    test_preds, test_trues = t.stack(test_preds), t.stack(test_trues)\n",
    "    test_time = time.time() - test_start\n",
    "    print(f\"Finished Final Test Evaluation for {loss_name} loss, Total runtime: {test_time:.2f}s\")\n",
    "\n",
    "    # Denormalize and evaluate t2m_f\n",
    "    preds_t2m = test_preds * t2m_f_std + t2m_f_mean\n",
    "    trues_t2m = test_trues * t2m_f_std + t2m_f_mean\n",
    "    mae_t2m = t.mean(t.abs(preds_t2m - trues_t2m)).item()\n",
    "    rmse_t2m = t.sqrt(t.mean((preds_t2m - trues_t2m) ** 2)).item()\n",
    "    print(f\"t2m_f L1 norm (°F): {mae_t2m:.2f}\")\n",
    "    print(f\"t2m_f RMSE (°F): {rmse_t2m:.2f}\")\n",
    "\n",
    "    # Denormalize sample outputs\n",
    "    sample_preds = preds_t2m[:5, 0, 0].numpy()\n",
    "    sample_targets = trues_t2m[:5, 0, 0].numpy()\n",
    "    print(f\"Sample t2m_f preds (°F): {sample_preds}\")\n",
    "    print(f\"Sample t2m_f targets (°F): {sample_targets}\")\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=f'Train Loss ({loss_name})', color='blue')\n",
    "    plt.plot([i * 5 for i in range(len(val_losses))], val_losses, label='Validation Loss', color='orange', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(f'{loss_name} Loss (Normalized)')\n",
    "    plt.title(f'Training and Validation Loss for t2m_f ({loss_name})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'loss_plot_t2m_f_{loss_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Time Series Plot for t2m_f\n",
    "    plot_start = time.time()\n",
    "    print(\"Starting Time Series Plotting for t2m_f\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(preds_t2m[:, 0, 0].numpy(), label='Pred t2m_f', color='red', linestyle='--')\n",
    "    plt.plot(trues_t2m[:, 0, 0].numpy(), label='True t2m_f', color='green')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Temperature (°F)')\n",
    "    plt.title('Predicted vs Actual t2m_f')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('t2m_f_timeseries.png')\n",
    "    print(f\"Finished Time Series Plotting, Total runtime: {time.time() - plot_start:.2f}s\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ecd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model state\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Loaded best model state based on validation loss.\")\n",
    "\n",
    "# Test evaluation\n",
    "test_start = time.time()\n",
    "print(\"Starting Final Test Evaluation\")\n",
    "model.eval()\n",
    "test_preds, test_trues = [], []\n",
    "with t.no_grad():\n",
    "    for t_step in range(temp_test_inputs.shape[0] - 1):\n",
    "        input_x = temp_test_inputs[t_step].reshape(num_nodes, -1)\n",
    "        target_y = temp_test_targets[t_step, :, 5, :].reshape(num_nodes, -1)\n",
    "        out = model(input_x, edge_index)\n",
    "        test_preds.append(out.cpu())\n",
    "        test_trues.append(target_y.cpu())\n",
    "test_preds, test_trues = t.stack(test_preds), t.stack(test_trues)\n",
    "test_time = time.time() - test_start\n",
    "print(f\"Finished Final Test Evaluation, Total runtime: {test_time:.2f}s\")\n",
    "\n",
    "# Denormalize and evaluate t2m_f\n",
    "preds_t2m = test_preds * t2m_f_std + t2m_f_mean\n",
    "trues_t2m = test_trues * t2m_f_std + t2m_f_mean\n",
    "mae_t2m = t.mean(t.abs(preds_t2m - trues_t2m)).item()\n",
    "rmse_t2m = t.sqrt(t.mean((preds_t2m - trues_t2m) ** 2)).item()\n",
    "print(f\"t2m_f L1 norm (°F): {mae_t2m:.2f}\")\n",
    "print(f\"t2m_f RMSE (°F): {rmse_t2m:.2f}\")\n",
    "\n",
    "# Denormalize sample outputs\n",
    "sample_preds = preds_t2m[:5, 0, 0].numpy()\n",
    "sample_targets = trues_t2m[:5, 0, 0].numpy()\n",
    "print(f\"Sample t2m_f preds (°F): {sample_preds}\")\n",
    "print(f\"Sample t2m_f targets (°F): {sample_targets}\")\n",
    "\n",
    "# Loss Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', color='blue')\n",
    "plt.plot(val_epochs, val_losses, label='Validation Loss', color='orange', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('L1 Loss (Normalized)')\n",
    "plt.title('Training and Validation Loss for t2m_f')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('loss_plot_t2m_f.png')\n",
    "plt.show()\n",
    "\n",
    "# Time Series Plot for t2m_f\n",
    "plot_start = time.time()\n",
    "print(\"Starting Time Series Plotting for t2m_f\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(preds_t2m[:, 0, 0].numpy(), label='Pred t2m_f', color='red', linestyle='--')\n",
    "plt.plot(trues_t2m[:, 0, 0].numpy(), label='True t2m_f', color='green')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Temperature (°F)')\n",
    "plt.title('Predicted vs Actual t2m_f')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('t2m_f_timeseries.png')\n",
    "print(f\"Finished Time Series Plotting, Total runtime: {time.time() - plot_start:.2f}s\")\n",
    "plt.show()\n",
    "\n",
    "# Testing function to load and test the model on new data\n",
    "def test_model(model_path, inputs_tensor, targets_tensor, edge_index, num_nodes, device, t2m_f_mean, t2m_f_std, lat_subset, lon_subset):\n",
    "    model = WeatherGNN(num_features=15, num_outputs=1).to(device)\n",
    "    model.load_state_dict(t.load(model_path))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "    test_preds, test_trues = [], []\n",
    "    with t.no_grad():\n",
    "        for t_step in range(inputs_tensor.shape[0] - 1):\n",
    "            input_x = inputs_tensor[t_step].reshape(num_nodes, -1)\n",
    "            target_y = targets_tensor[t_step, :, 5, :].reshape(num_nodes, -1)\n",
    "            out = model(input_x, edge_index)\n",
    "            test_preds.append(out.cpu())\n",
    "            test_trues.append(target_y.cpu())\n",
    "    test_preds, test_trues = t.stack(test_preds), t.stack(test_trues)\n",
    "\n",
    "    # Denormalize and evaluate t2m_f\n",
    "    preds_t2m = test_preds * t2m_f_std + t2m_f_mean\n",
    "    trues_t2m = test_trues * t2m_f_std + t2m_f_mean\n",
    "    mae_t2m = t.mean(t.abs(preds_t2m - trues_t2m)).item()\n",
    "    rmse_t2m = t.sqrt(t.mean((preds_t2m - trues_t2m) ** 2)).item()\n",
    "    print(f\"t2m_f L1 norm (°F): {mae_t2m:.2f}\")\n",
    "    print(f\"t2m_f RMSE (°F): {rmse_t2m:.2f}\")\n",
    "\n",
    "    # Plot variation at specific nodes\n",
    "    nodes_to_plot = [0, 236, 8926, 23500]  # Example: NW, NE, Central, SW\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for idx in nodes_to_plot:\n",
    "        plt.plot(preds_t2m[:, idx, 0].numpy(), label=f'Pred Node {idx}', linestyle='--')\n",
    "        plt.plot(trues_t2m[:, idx, 0].numpy(), label=f'True Node {idx}', linestyle='-')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Temperature (°F)')\n",
    "    plt.title('Predicted vs True t2m_f at Selected Nodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('t2m_f_variation_nodes.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot spatial difference for first time step\n",
    "    first_pred = preds_t2m[0, :, 0].cpu().numpy()\n",
    "    first_true = trues_t2m[0, :, 0].cpu().numpy()\n",
    "    diff = np.abs(first_pred - first_true)\n",
    "    diff_grid = diff.reshape(101, 237)\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.contourf(lon_subset, lat_subset, diff_grid, cmap='RdBu_r', levels=np.linspace(0, 5, 21))\n",
    "    plt.colorbar(label='Absolute Difference (°F)')\n",
    "    plt.title('Absolute Difference (Pred - True) for t2m_f at First Test Time Step')\n",
    "    plt.xlabel('Longitude (°E)')\n",
    "    plt.ylabel('Latitude (°N)')\n",
    "    plt.savefig('t2m_f_grid_difference.png')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage of test_model (uncomment to use with new data)\n",
    "# test_model('best_weather_gnn_model.pth', new_inputs_tensor, new_targets_tensor, edge_index, num_nodes, device, t2m_f_mean, t2m_f_std, lat_subset, lon_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc13650",
   "metadata": {},
   "source": [
    "# Early Testing and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46780e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function to load and test the model on test data\n",
    "def test_model(model_path, inputs_tensor, targets_tensor, edge_index, num_nodes, device, t2m_f_mean, t2m_f_std, lat_subset, lon_subset):\n",
    "    model = WeatherGNN(num_features=15, num_outputs=1).to(device)\n",
    "    model.load_state_dict(t.load(model_path))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "    # Validate inputs\n",
    "    print(f\"Inputs tensor shape: {inputs_tensor.shape}\")\n",
    "    print(f\"Targets tensor shape: {targets_tensor.shape}\")\n",
    "    if inputs_tensor.shape[0] < 2 or targets_tensor.shape[0] < 2:\n",
    "        raise ValueError(\"Input and target tensors must have at least 2 time steps for prediction.\")\n",
    "\n",
    "    test_preds, test_trues = [], []\n",
    "    with t.no_grad():\n",
    "        for t_step in range(inputs_tensor.shape[0] - 1):\n",
    "            input_x = inputs_tensor[t_step].reshape(num_nodes, -1)\n",
    "            target_y = targets_tensor[t_step, :, 5, :].reshape(num_nodes, -1)\n",
    "            out = model(input_x, edge_index)\n",
    "            test_preds.append(out.cpu())\n",
    "            test_trues.append(target_y.cpu())\n",
    "    test_preds, test_trues = t.stack(test_preds), t.stack(test_trues)\n",
    "\n",
    "    # Denormalize and evaluate t2m_f\n",
    "    preds_t2m = test_preds * t2m_f_std + t2m_f_mean\n",
    "    trues_t2m = test_trues * t2m_f_std + t2m_f_mean\n",
    "    mae_t2m = t.mean(t.abs(preds_t2m - trues_t2m)).item()\n",
    "    rmse_t2m = t.sqrt(t.mean((preds_t2m - trues_t2m) ** 2)).item()\n",
    "    print(f\"t2m_f L1 norm (°F): {mae_t2m:.2f}\")\n",
    "    print(f\"t2m_f RMSE (°F): {rmse_t2m:.2f}\")\n",
    "\n",
    "    # Plot variation at specific nodes\n",
    "    nodes_to_plot = [0, 236, 8926, 23500]  # Example: NW, NE, Central, SW\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for idx in nodes_to_plot:\n",
    "        plt.plot(preds_t2m[:, idx, 0].numpy(), label=f'Pred Node {idx}', linestyle='--')\n",
    "        plt.plot(trues_t2m[:, idx, 0].numpy(), label=f'True Node {idx}', linestyle='-')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Temperature (°F)')\n",
    "    plt.title('Predicted vs True t2m_f at Selected Nodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('t2m_f_variation_nodes.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Debug and plot spatial difference for first time step\n",
    "    first_pred = preds_t2m[0, :, 0].cpu().numpy()\n",
    "    first_true = trues_t2m[0, :, 0].cpu().numpy()\n",
    "    print(f\"First predicted t2m_f shape: {first_pred.shape}\")\n",
    "    print(f\"First true t2m_f shape: {first_true.shape}\")\n",
    "    print(f\"First predicted t2m_f sample: {first_pred[:5]}\")\n",
    "    print(f\"First true t2m_f sample: {first_true[:5]}\")\n",
    "\n",
    "    # Check for NaN or infinite values\n",
    "    if np.any(np.isnan(first_pred)) or np.any(np.isnan(first_true)):\n",
    "        raise ValueError(\"NaN values detected in predictions or true values.\")\n",
    "    if np.any(np.isinf(first_pred)) or np.any(np.isinf(first_true)):\n",
    "        raise ValueError(\"Infinite values detected in predictions or true values.\")\n",
    "\n",
    "    diff = np.abs(first_pred - first_true)\n",
    "    print(f\"Difference min: {np.min(diff):.2f}, max: {np.max(diff):.2f}, mean: {np.mean(diff):.2f}\")\n",
    "\n",
    "    # Reshape difference to 2D grid\n",
    "    if diff.shape[0] != num_nodes:\n",
    "        raise ValueError(f\"Expected {num_nodes} nodes, but got {diff.shape[0]}\")\n",
    "    if len(lat_subset) * len(lon_subset) != num_nodes:\n",
    "        raise ValueError(f\"Grid size mismatch: lat_subset ({len(lat_subset)}) * lon_subset ({len(lon_subset)}) != num_nodes ({num_nodes})\")\n",
    "    diff_grid = diff.reshape(len(lat_subset), len(lon_subset))\n",
    "\n",
    "    # Ensure matplotlib backend is set for inline display\n",
    "    try:\n",
    "        import matplotlib\n",
    "        matplotlib.use('Agg')  # Use a non-interactive backend for saving\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting matplotlib backend: {e}\")\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    contour = plt.contourf(lon_subset, lat_subset, diff_grid, cmap='RdBu_r', levels=np.linspace(0, max(np.max(diff), 1), 21))\n",
    "    plt.colorbar(label='Absolute Difference (°F)')\n",
    "    plt.title('Absolute Difference (Pred - True) for t2m_f at First Test Time Step')\n",
    "    plt.xlabel('Longitude (°E)')\n",
    "    plt.ylabel('Latitude (°N)')\n",
    "    \n",
    "    # Save the plot\n",
    "    save_path = 't2m_f_grid_difference.png'\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Spatial grid plot saved to: {os.path.abspath(save_path)}\")\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Run the test on the test dataset\n",
    "test_model(\n",
    "    model_path=r'f:\\weather_forecasting\\notebooks\\final project\\models\\paths\\best_weather_gnn_model_ext_training.pth',\n",
    "    inputs_tensor=temp_test_inputs,\n",
    "    targets_tensor=temp_test_targets,\n",
    "    edge_index=edge_index,\n",
    "    num_nodes=num_nodes,\n",
    "    device=device,\n",
    "    t2m_f_mean=t2m_f_mean,\n",
    "    t2m_f_std=t2m_f_std,\n",
    "    lat_subset=lat_subset,\n",
    "    lon_subset=lon_subset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489091de",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc4988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import gc\n",
    "import os\n",
    "\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define directory for saving the model\n",
    "model_save_dir = r'F:\\weather_forecasting\\notebooks\\final project\\models\\Final Model'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Load tensors\n",
    "train_inputs_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_inputs_norm.pt').to(device)\n",
    "train_targets_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_targets_norm.pt').to(device)\n",
    "print(f\"Tensors loaded: train inputs {train_inputs_tensor.shape}, train targets {train_targets_tensor.shape}\")\n",
    "print(f\"Normalized t2m_f mean: {train_targets_tensor[:, :, 5, :].mean().item():.2f}, std: {train_targets_tensor[:, :, 5, :].std().item():.2f}\")\n",
    "\n",
    "# Edge index (computed on CPU, then moved to GPU)\n",
    "num_nodes = 23937\n",
    "k = 8\n",
    "lat_subset = np.linspace(50, 25, 101)\n",
    "lon_subset = np.linspace(235, 294, 237)\n",
    "coords = np.stack(np.meshgrid(lat_subset, lon_subset, indexing='ij'), axis=-1).reshape(-1, 2)\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1).fit(coords)\n",
    "_, indices = nbrs.kneighbors(coords)\n",
    "edge_index = t.tensor(np.stack([np.repeat(np.arange(num_nodes), k), indices[:, 1:].flatten()]), dtype=t.long).to(device)\n",
    "\n",
    "# Model (simplified GNN without temporal layer)\n",
    "class WeatherGNN(t.nn.Module):\n",
    "    def __init__(self, num_features=15, hidden_dims=128, num_outputs=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dims)\n",
    "        self.conv2 = GCNConv(hidden_dims, hidden_dims)\n",
    "        self.conv3 = GCNConv(hidden_dims, num_outputs)\n",
    "        self.dropout = t.nn.Dropout(0.3)\n",
    "        self.residual = t.nn.Linear(num_features, num_outputs)\n",
    "        self.res_weight = t.nn.Parameter(t.tensor(2.0))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        residual = self.residual(x) * self.res_weight\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x + residual\n",
    "\n",
    "# Define L1 loss function directly\n",
    "def l1_loss(x, y):\n",
    "    return t.mean(t.abs(x - y))\n",
    "\n",
    "# Pre-compute t2m_f statistics for denormalization\n",
    "t2m_f_std = train_targets_tensor[:, :, 5, :].std().item()\n",
    "t2m_f_mean = train_targets_tensor[:, :, 5, :].mean().item()\n",
    "t2m_f_mean = 42.36  # °F\n",
    "t2m_f_std = 21.75   # °F\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 10\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(\"\\nTraining with L1 loss for t2m_f prediction...\")\n",
    "t.cuda.empty_cache()\n",
    "gc.collect()\n",
    "model = WeatherGNN(num_features=15, hidden_dims=128, num_outputs=1).to(device)\n",
    "optimizer = t.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = t.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"Starting Epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for t_step in range(train_inputs_tensor.shape[0] - 1):\n",
    "        input_x = train_inputs_tensor[t_step].reshape(num_nodes, -1)\n",
    "        target_y = train_targets_tensor[t_step, :, 5, :].reshape(num_nodes, -1)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(input_x, edge_index)\n",
    "        loss = l1_loss(out, target_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / (train_inputs_tensor.shape[0] - 1)\n",
    "    train_losses.append(avg_loss)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"Finished Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}, Total runtime: {epoch_time:.2f}s\")\n",
    "\n",
    "# Save the trained model\n",
    "model_path = os.path.join(model_save_dir, f'final_model.pth')\n",
    "t.save(model.state_dict(), model_path)\n",
    "print(f\"Saved model to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import gc\n",
    "import os\n",
    "\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define directory for saving the model\n",
    "model_save_dir = r'F:\\weather_forecasting\\notebooks\\final project\\models\\Final Model'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Load tensors\n",
    "train_inputs_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_inputs_norm.pt').to(device)\n",
    "train_targets_tensor = t.load('f:/weather_forecasting/notebooks/final project/tensors/train_targets_norm.pt').to(device)\n",
    "print(f\"Tensors loaded: train inputs {train_inputs_tensor.shape}, train targets {train_targets_tensor.shape}\")\n",
    "print(f\"Normalized t2m_f mean: {train_targets_tensor[:, :, 5, :].mean().item():.2f}, std: {train_targets_tensor[:, :, 5, :].std().item():.2f}\")\n",
    "\n",
    "# Edge index (computed on CPU, then moved to GPU)\n",
    "num_nodes = 23937\n",
    "k = 8\n",
    "lat_subset = np.linspace(50, 25, 101)\n",
    "lon_subset = np.linspace(235, 294, 237)\n",
    "coords = np.stack(np.meshgrid(lat_subset, lon_subset, indexing='ij'), axis=-1).reshape(-1, 2)\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1).fit(coords)\n",
    "_, indices = nbrs.kneighbors(coords)\n",
    "edge_index = t.tensor(np.stack([np.repeat(np.arange(num_nodes), k), indices[:, 1:].flatten()]), dtype=t.long).to(device)\n",
    "\n",
    "# Model (simplified GNN without temporal layer)\n",
    "class WeatherGNN(t.nn.Module):\n",
    "    def __init__(self, num_features=15, hidden_dims=128, num_outputs=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dims)\n",
    "        self.conv2 = GCNConv(hidden_dims, hidden_dims)\n",
    "        self.conv3 = GCNConv(hidden_dims, num_outputs)\n",
    "        self.dropout = t.nn.Dropout(0.3)\n",
    "        self.residual = t.nn.Linear(num_features, num_outputs)\n",
    "        self.res_weight = t.nn.Parameter(t.tensor(2.0))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        residual = self.residual(x) * self.res_weight\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x + residual\n",
    "\n",
    "# Define L1 loss function directly\n",
    "def l1_loss(x, y):\n",
    "    return t.mean(t.abs(x - y))\n",
    "\n",
    "# Pre-compute t2m_f statistics for denormalization\n",
    "t2m_f_std = train_targets_tensor[:, :, 5, :].std().item()\n",
    "t2m_f_mean = train_targets_tensor[:, :, 5, :].mean().item()\n",
    "t2m_f_mean = 42.36  # °F\n",
    "t2m_f_std = 21.75   # °F\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 35\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(\"\\nTraining with L1 loss for t2m_f prediction...\")\n",
    "t.cuda.empty_cache()\n",
    "gc.collect()\n",
    "model = WeatherGNN(num_features=15, hidden_dims=128, num_outputs=1).to(device)\n",
    "optimizer = t.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = t.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"Starting Epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for t_step in range(train_inputs_tensor.shape[0] - 1):\n",
    "        input_x = train_inputs_tensor[t_step].reshape(num_nodes, -1)\n",
    "        target_y = train_targets_tensor[t_step, :, 5, :].reshape(num_nodes, -1)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(input_x, edge_index)\n",
    "        loss = l1_loss(out, target_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / (train_inputs_tensor.shape[0] - 1)\n",
    "    train_losses.append(avg_loss)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"Finished Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}, Total runtime: {epoch_time:.2f}s\")\n",
    "\n",
    "# Save the trained model\n",
    "model_path = os.path.join(model_save_dir, f'final_model_ext_training.pth')\n",
    "t.save(model.state_dict(), model_path)\n",
    "print(f\"Saved model to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294327dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
