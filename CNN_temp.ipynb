{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7j2CtjfGJYR"
      },
      "outputs": [],
      "source": [
        "#Installing packages needed\n",
        "\n",
        "!pip install netcdf4\n",
        "!pip install torch_geometric\n",
        "import torch\n",
        "print(\"torch version is\", torch.__version__) #torch-2.6.0+cu124\n",
        "\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.6.0+cu124.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing and setting file paths\n",
        "import xarray as xr\n",
        "import numpy as np\n",
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import netCDF4 as nc\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import gc\n",
        "import torch_sparse\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "\n",
        "#Making sure can use data in Drive. Change this and make it suitable for Github\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/Weather Forecasting/Aash_work')\n",
        "#!ls /content/drive/MyDrive\n",
        "\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "#Add api request for data on"
      ],
      "metadata": {
        "id": "3J48zukwGTOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xarray as xr\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import psutil  # For memory usage monitoring\n",
        "\n",
        "# Define file paths\n",
        "file_path_train_p = \"Data/FinalData/p.nc\"\n",
        "file_path_train_a = \"Data/FinalData/a.nc\"\n",
        "file_path_train_i = \"Data/FinalData/i.nc\"\n",
        "output_path = \"Data/FinalData/combined_3.nc\"\n",
        "history_path = \"training_history.csv\"\n",
        "plot_path_prefix = \"prediction_plot\"\n",
        "\n",
        "# Function to print memory usage\n",
        "def print_memory_usage():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    mem_info = process.memory_info()\n",
        "    print(f\"Memory usage: {mem_info.rss / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "# Step 1: Merge .nc files with checks\n",
        "def merge_nc_files(file_paths, output_path, time_subset=None):\n",
        "    # Print memory usage before merging\n",
        "    print(\"Memory usage before merging:\")\n",
        "    print_memory_usage()\n",
        "\n",
        "    datasets = []\n",
        "    for fp in file_paths:\n",
        "        # Check if file exists\n",
        "        if not os.path.exists(fp):\n",
        "            raise FileNotFoundError(f\"File not found: {fp}\")\n",
        "        # Load dataset with chunking (use valid_time instead of time)\n",
        "        ds = xr.open_dataset(fp, chunks={'valid_time': 100})\n",
        "        # Print dataset info for debugging\n",
        "        print(f\"\\nDataset from {fp}:\")\n",
        "        print(f\"Dimensions: {ds.dims}\")\n",
        "        print(f\"Coordinates: {list(ds.coords.keys())}\")\n",
        "        print(f\"Variables: {list(ds.variables.keys())}\")\n",
        "        # Print expver values if present\n",
        "        if 'expver' in ds.variables:\n",
        "            print(f\"{fp}: expver values = {ds['expver'].values}\")\n",
        "        # Drop expver to avoid merge conflicts\n",
        "        if 'expver' in ds.variables:\n",
        "            ds = ds.drop_vars('expver')\n",
        "        # Subset time dimension if specified\n",
        "        if time_subset is not None:\n",
        "            ds = ds.isel(valid_time=slice(0, time_subset))  # Updated to valid_time\n",
        "        datasets.append(ds)\n",
        "\n",
        "    # Check for consistent dimensions and coordinates\n",
        "    dims = [ds.dims for ds in datasets]\n",
        "    coords = [ds.coords.keys() for ds in datasets]\n",
        "    dims_set = set(tuple(sorted(d.items())) for d in dims)\n",
        "    if len(dims_set) > 1:\n",
        "        print(\"Warning: Datasets have different dimensions:\", [dict(d) for d in dims])\n",
        "    coords_set = set(tuple(sorted(c)) for c in coords)\n",
        "    if len(coords_set) > 1:\n",
        "        print(\"Warning: Datasets have different coordinates:\", coords)\n",
        "\n",
        "    # Merge datasets\n",
        "    try:\n",
        "        combined = xr.merge(datasets, compat='no_conflicts')\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to merge datasets: {str(e)}\")\n",
        "\n",
        "    # Check for 't2m' in combined dataset\n",
        "    if 't2m' not in combined.variables:\n",
        "        raise ValueError(\"'t2m' variable not found in combined dataset\")\n",
        "\n",
        "    # Check for missing values in 't2m'\n",
        "    if combined['t2m'].isnull().any():\n",
        "        print(\"Warning: 't2m' in combined dataset contains missing values\")\n",
        "\n",
        "    # Check for missing values in other variables\n",
        "    for var in combined.variables:\n",
        "        if combined[var].isnull().any():\n",
        "            print(f\"Warning: Variable '{var}' contains missing values\")\n",
        "\n",
        "    # Save combined dataset\n",
        "    combined.to_netcdf(output_path)\n",
        "    print(f\"Merged file saved to {output_path}\")\n",
        "\n",
        "    # Check merged file size\n",
        "    if os.path.exists(output_path):\n",
        "        file_size_mb = os.path.getsize(output_path) / (1024 * 1024)  # Convert bytes to MB\n",
        "        print(f\"Merged file size: {file_size_mb:.2f} MB\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Merged file not found at {output_path}\")\n",
        "\n",
        "    # Print memory usage after merging\n",
        "    print(\"Memory usage after merging:\")\n",
        "    print_memory_usage()\n",
        "\n",
        "    return combined\n",
        "\n",
        "# Merge files with checks\n",
        "file_paths = [file_path_train_p, file_path_train_a, file_path_train_i]\n",
        "combined_ds = merge_nc_files(file_paths, output_path, time_subset=None)\n",
        "\n",
        "# Step 2: Prepare data for CNN with checks\n",
        "def prepare_data(dataset):\n",
        "    # Print memory usage before loading data\n",
        "    print(\"Memory usage before loading data:\")\n",
        "    print_memory_usage()\n",
        "\n",
        "    # Print dataset info for debugging\n",
        "    print(\"\\nCombined dataset info:\")\n",
        "    print(f\"Dimensions: {dataset.dims}\")\n",
        "    print(f\"Coordinates: {list(dataset.coords.keys())}\")\n",
        "    print(f\"Variables: {list(dataset.variables.keys())}\")\n",
        "\n",
        "    # Check if 't2m' exists\n",
        "    if 't2m' not in dataset.variables:\n",
        "        raise ValueError(\"'t2m' variable not found in combined dataset\")\n",
        "\n",
        "    # Handle pressure_level dimension if present\n",
        "    if 'pressure_level' in dataset.dims:\n",
        "        print(f\"Pressure levels found: {dataset['pressure_level'].values}\")\n",
        "        # Since t2m is a surface variable, it shouldn't have pressure levels\n",
        "        # Check t2m dimensions\n",
        "        t2m_dims = dataset['t2m'].dims\n",
        "        print(f\"t2m dimensions: {t2m_dims}\")\n",
        "        if 'pressure_level' in t2m_dims:\n",
        "            raise ValueError(\"t2m should not have a pressure_level dimension; check dataset structure\")\n",
        "\n",
        "    # Use chunks to reduce memory usage (use valid_time instead of time)\n",
        "    chunk_size = {'valid_time': 100}  # Adjust based on your dataset size\n",
        "    dataset = dataset.chunk(chunk_size)\n",
        "\n",
        "    # Extract temperature and additional variables\n",
        "    temp = dataset['t2m'].values  # Shape: (valid_time, latitude, longitude)\n",
        "\n",
        "    # Check for valid data shape\n",
        "    if len(temp.shape) != 3:\n",
        "        raise ValueError(f\"Expected 3D temperature array (valid_time, latitude, longitude), got shape {temp.shape}\")\n",
        "\n",
        "    # Check for NaN or infinite values\n",
        "    if np.any(np.isnan(temp)) or np.any(np.isinf(temp)):\n",
        "        raise ValueError(\"Temperature data contains NaN or infinite values\")\n",
        "\n",
        "    # Check for sufficient data\n",
        "    if temp.shape[0] < 2:\n",
        "        raise ValueError(\"Not enough time steps for training (need at least 2)\")\n",
        "\n",
        "    # Print dataset size\n",
        "    print(f\"Dataset size: {temp.shape[0]} time steps, {temp.shape[1]} lat, {temp.shape[2]} lon\")\n",
        "\n",
        "    # Normalize temperature (scale to 0-1)\n",
        "    temp_min, temp_max = np.min(temp), np.max(temp)\n",
        "    if temp_max == temp_min:\n",
        "        raise ValueError(\"Temperature data has no variation (min equals max)\")\n",
        "    temp_normalized = (temp - temp_min) / (temp_max - temp_min)\n",
        "\n",
        "    # Verify normalization\n",
        "    if not (np.min(temp_normalized) >= 0 and np.max(temp_normalized) <= 1):\n",
        "        raise ValueError(\"Normalization failed: values outside [0, 1]\")\n",
        "\n",
        "    # Prepare additional variables (e.g., 'tp', 'msl', 'u10', 'v10')\n",
        "    input_channels = [temp_normalized[..., np.newaxis]]  # Start with t2m\n",
        "    additional_vars = ['tp', 'msl', 'u10', 'v10']\n",
        "    for var in additional_vars:\n",
        "        if var in dataset.variables:\n",
        "            var_data = dataset[var].values\n",
        "            if 'pressure_level' in dataset[var].dims:\n",
        "                print(f\"Skipping variable '{var}' due to pressure_level dimension\")\n",
        "                continue\n",
        "            if var_data.shape != temp.shape:\n",
        "                print(f\"Warning: Variable '{var}' shape {var_data.shape} does not match t2m shape {temp.shape}\")\n",
        "                continue\n",
        "            if np.any(np.isnan(var_data)) or np.any(np.isinf(var_data)):\n",
        "                print(f\"Warning: Variable '{var}' contains NaN or infinite values\")\n",
        "                continue\n",
        "            var_min, var_max = np.min(var_data), np.max(var_data)\n",
        "            if var_max == var_min:\n",
        "                print(f\"Warning: Variable '{var}' has no variation\")\n",
        "                continue\n",
        "            var_normalized = (var_data - var_min) / (var_max - var_min)\n",
        "            input_channels.append(var_normalized[..., np.newaxis])\n",
        "\n",
        "    # Stack input channels\n",
        "    X = np.concatenate(input_channels, axis=-1)  # Shape: (valid_time, latitude, longitude, channels)\n",
        "\n",
        "    # Check input shape\n",
        "    print(f\"Input shape (with {X.shape[-1]} channels): {X.shape}\")\n",
        "\n",
        "    # Create input-output pairs (predict next time step)\n",
        "    X = X[:-1]  # All but last time step\n",
        "    y = temp_normalized[1:]  # t2m only, all but first time step\n",
        "\n",
        "    # Check X and y consistency\n",
        "    if X.shape[0] != y.shape[0]:\n",
        "        raise ValueError(f\"Mismatch in X and y samples: {X.shape[0]} vs {y.shape[0]}\")\n",
        "\n",
        "    # Split into train and test (80-20 split)\n",
        "    train_size = int(0.8 * len(X))\n",
        "    if train_size == 0:\n",
        "        raise ValueError(\"Training set is empty after split\")\n",
        "\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "    # Verify train/test shapes\n",
        "    print(f\"X_train shape: {X_train.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}\")\n",
        "    print(f\"y_train shape: {y_train.shape}\")\n",
        "    print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "    # Print memory usage after loading data\n",
        "    print(\"Memory usage after loading data:\")\n",
        "    print_memory_usage()\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, temp_min, temp_max\n",
        "\n",
        "# Prepare data with checks\n",
        "X_train, X_test, y_train, y_test, temp_min, temp_max = prepare_data(combined_ds)\n",
        "\n",
        "# Step 3: Build CNN model with two additional hidden layers\n",
        "def build_cnn(input_shape):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu', kernel_regularizer=l2(0.005)),\n",
        "        Dropout(0.4),\n",
        "        Dense(128, activation='relu', kernel_regularizer=l2(0.005)),\n",
        "        Dropout(0.4),\n",
        "        Dense(64, activation='relu', kernel_regularizer=l2(0.005)),\n",
        "        Dense(np.prod(y_train.shape[1:]), activation='linear')\n",
        "    ])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# Build and train model\n",
        "input_shape = X_train.shape[1:]  # (lat, lon, channels)\n",
        "model = build_cnn(input_shape)\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    X_train, y_train.reshape(-1, np.prod(y_train.shape[1:])),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test, y_test.reshape(-1, np.prod(y_test.shape[1:]))),\n",
        "    callbacks=[early_stopping, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Step 4: Save model and history\n",
        "model.save('temperature_cnn_model.keras')\n",
        "print(\"Model saved to temperature_cnn_model.keras\")\n",
        "\n",
        "# Save training history\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.to_csv(history_path, index=False)\n",
        "print(f\"Training history saved to {history_path}\")\n",
        "\n",
        "# Step 5: Predict and denormalize\n",
        "def predict_temperature(model, X, temp_min, temp_max):\n",
        "    pred_normalized = model.predict(X)\n",
        "    # Clip predictions to [0, 1] to match normalization range\n",
        "    pred_normalized = np.clip(pred_normalized, 0, 1)\n",
        "    pred_reshaped = pred_normalized.reshape(X.shape[:-1])  # Remove channel dimension\n",
        "    pred_denorm = pred_reshaped * (temp_max - temp_min) + temp_min\n",
        "    return pred_denorm\n",
        "\n",
        "# Get predictions\n",
        "y_pred = predict_temperature(model, X_test, temp_min, temp_max)\n",
        "y_true = y_test * (temp_max - temp_min) + temp_min  # Denormalize true values\n",
        "\n",
        "# Step 6: Plot predictions\n",
        "# Plot 1: Spatial map for a sample test time step\n",
        "sample_idx = 0\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Predicted t2m\")\n",
        "plt.imshow(y_pred[sample_idx], cmap='coolwarm')\n",
        "plt.colorbar(label='Temperature (K)')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Actual t2m\")\n",
        "plt.imshow(y_true[sample_idx], cmap='coolwarm')\n",
        "plt.colorbar(label='Temperature (K)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{plot_path_prefix}_spatial.png\")\n",
        "plt.close()\n",
        "print(f\"Spatial plot saved to {plot_path_prefix}_spatial.png\")\n",
        "\n",
        "# Plot 2: Time series of mean t2m\n",
        "mean_pred = np.mean(y_pred, axis=(1, 2))\n",
        "mean_true = np.mean(y_true, axis=(1, 2))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(mean_pred, label='Predicted Mean t2m', color='blue')\n",
        "plt.plot(mean_true, label='Actual Mean t2m', color='red')\n",
        "plt.title(\"Mean t2m Over Time (Test Set)\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"Temperature (K)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(f\"{plot_path_prefix}_timeseries.png\")\n",
        "plt.close()\n",
        "print(f\"Time series plot saved to {plot_path_prefix}_timeseries.png\")\n",
        "\n",
        "# Plot 3: Difference map for a sample test time step\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.title(\"Prediction Error (Predicted - Actual t2m)\")\n",
        "plt.imshow(y_pred[sample_idx] - y_true[sample_idx], cmap='coolwarm', vmin=-5, vmax=5)\n",
        "plt.colorbar(label='Error (K)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{plot_path_prefix}_difference.png\")\n",
        "plt.close()\n",
        "print(f\"Difference plot saved to {plot_path_prefix}_difference.png\")\n",
        "\n",
        "# Plot 4: Scatter plot of predicted vs actual mean t2m\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(mean_true, mean_pred, alpha=0.5)\n",
        "plt.plot([mean_true.min(), mean_true.max()], [mean_true.min(), mean_true.max()], 'k--')\n",
        "plt.title(\"Predicted vs Actual Mean t2m\")\n",
        "plt.xlabel(\"Actual Mean t2m (K)\")\n",
        "plt.ylabel(\"Predicted Mean t2m (K)\")\n",
        "plt.grid(True)\n",
        "plt.savefig(f\"{plot_path_prefix}_scatter.png\")\n",
        "plt.close()\n",
        "print(f\"Scatter plot saved to {plot_path_prefix}_scatter.png\")\n",
        "\n",
        "# Step 7: Comparison of Predictions and Actual Values\n",
        "# Compute metrics\n",
        "mae = np.mean(np.abs(y_pred - y_true))\n",
        "rmse = np.sqrt(np.mean((y_pred - y_true) ** 2))\n",
        "mbe = np.mean(y_pred - y_true)\n",
        "\n",
        "print(\"\\nPrediction Metrics:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f} K\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f} K\")\n",
        "print(f\"Mean Bias Error (MBE): {mbe:.4f} K\")\n",
        "\n",
        "# Plot 5: Loss curve from training history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history_df['loss'], label='Training Loss')\n",
        "plt.plot(history_df['val_loss'], label='Validation Loss')\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss (MSE)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(f\"{plot_path_prefix}_loss_curve.png\")\n",
        "plt.close()\n",
        "print(f\"Loss curve plot saved to {plot_path_prefix}_loss_curve.png\")\n",
        "\n",
        "# Plot 6: Histogram of prediction errors\n",
        "errors = (y_pred - y_true).flatten()\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(errors, bins=50, edgecolor='black')\n",
        "plt.title(\"Histogram of Prediction Errors\")\n",
        "plt.xlabel(\"Error (K)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.savefig(f\"{plot_path_prefix}_error_histogram.png\")\n",
        "plt.close()\n",
        "print(f\"Error histogram plot saved to {plot_path_prefix}_error_histogram.png\")\n",
        "\n",
        "# Optional: Evaluate model\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test.reshape(-1, np.prod(y_test.shape[1:])))\n",
        "print(f\"Test MAE (normalized scale): {test_mae:.4f}\")\n",
        "print(\"Sample prediction shape:\", y_pred.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3PG4pP0YG-h",
        "outputId": "9e5d0819-6c42-4eee-b803-43b6c4355396"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory usage before merging:\n",
            "Memory usage: 1296.32 MB\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-3adb06932af2>:39: UserWarning: The specified chunks separate the stored chunks along dimension \"valid_time\" starting at index 100. This could degrade performance. Instead, consider rechunking after loading.\n",
            "  ds = xr.open_dataset(fp, chunks={'valid_time': 100})\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset from Data/FinalData/p.nc:\n",
            "Dimensions: FrozenMappingWarningOnValuesAccess({'valid_time': 2160, 'pressure_level': 1, 'latitude': 101, 'longitude': 237})\n",
            "Coordinates: ['number', 'valid_time', 'pressure_level', 'latitude', 'longitude', 'expver']\n",
            "Variables: ['number', 'valid_time', 'pressure_level', 'latitude', 'longitude', 'expver', 'q', 't']\n",
            "Data/FinalData/p.nc: expver values = ['0001' '0001' '0001' ... '0005' '0005' '0005']\n",
            "\n",
            "Dataset from Data/FinalData/a.nc:\n",
            "Dimensions: FrozenMappingWarningOnValuesAccess({'valid_time': 2160, 'latitude': 101, 'longitude': 237})\n",
            "Coordinates: ['number', 'valid_time', 'latitude', 'longitude', 'expver']\n",
            "Variables: ['number', 'valid_time', 'latitude', 'longitude', 'expver', 'tp', 'slhf', 'sshf', 'ssrd', 'strd']\n",
            "Data/FinalData/a.nc: expver values = ['0001' '0001' '0001' ... '0005' '0005' '0005']\n",
            "\n",
            "Dataset from Data/FinalData/i.nc:\n",
            "Dimensions: FrozenMappingWarningOnValuesAccess({'valid_time': 2160, 'latitude': 101, 'longitude': 237})\n",
            "Coordinates: ['number', 'valid_time', 'latitude', 'longitude', 'expver']\n",
            "Variables: ['number', 'valid_time', 'latitude', 'longitude', 'expver', 'u10', 'v10', 'd2m', 't2m', 'sp', 'tcc', 'stl1', 'blh']\n",
            "Data/FinalData/i.nc: expver values = ['0001' '0001' '0001' ... '0005' '0005' '0005']\n",
            "Warning: Datasets have different dimensions: [{'valid_time': 2160, 'pressure_level': 1, 'latitude': 101, 'longitude': 237}, {'valid_time': 2160, 'latitude': 101, 'longitude': 237}, {'valid_time': 2160, 'latitude': 101, 'longitude': 237}]\n",
            "Warning: Datasets have different coordinates: [KeysView(Coordinates:\n",
            "    number          int64 8B ...\n",
            "  * valid_time      (valid_time) datetime64[ns] 17kB 2025-01-01 ... 2025-03-3...\n",
            "  * pressure_level  (pressure_level) float64 8B 850.0\n",
            "  * latitude        (latitude) float64 808B 50.0 49.75 49.5 ... 25.5 25.25 25.0\n",
            "  * longitude       (longitude) float64 2kB -125.0 -124.8 ... -66.25 -66.0), KeysView(Coordinates:\n",
            "    number      int64 8B ...\n",
            "  * valid_time  (valid_time) datetime64[ns] 17kB 2025-01-01 ... 2025-03-31T23...\n",
            "  * latitude    (latitude) float64 808B 50.0 49.75 49.5 ... 25.5 25.25 25.0\n",
            "  * longitude   (longitude) float64 2kB -125.0 -124.8 -124.5 ... -66.25 -66.0), KeysView(Coordinates:\n",
            "    number      int64 8B ...\n",
            "  * valid_time  (valid_time) datetime64[ns] 17kB 2025-01-01 ... 2025-03-31T23...\n",
            "  * latitude    (latitude) float64 808B 50.0 49.75 49.5 ... 25.5 25.25 25.0\n",
            "  * longitude   (longitude) float64 2kB -125.0 -124.8 -124.5 ... -66.25 -66.0)]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-3adb06932af2>:39: UserWarning: The specified chunks separate the stored chunks along dimension \"valid_time\" starting at index 100. This could degrade performance. Instead, consider rechunking after loading.\n",
            "  ds = xr.open_dataset(fp, chunks={'valid_time': 100})\n",
            "<ipython-input-4-3adb06932af2>:39: UserWarning: The specified chunks separate the stored chunks along dimension \"valid_time\" starting at index 100. This could degrade performance. Instead, consider rechunking after loading.\n",
            "  ds = xr.open_dataset(fp, chunks={'valid_time': 100})\n",
            "<ipython-input-4-3adb06932af2>:59: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
            "  dims_set = set(tuple(sorted(d.items())) for d in dims)\n",
            "<frozen _collections_abc>:861: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
            "<ipython-input-4-3adb06932af2>:61: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
            "  print(\"Warning: Datasets have different dimensions:\", [dict(d) for d in dims])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged file saved to Data/FinalData/combined_3.nc\n",
            "Merged file size: 1331.78 MB\n",
            "Memory usage after merging:\n",
            "Memory usage: 2082.54 MB\n",
            "Memory usage before loading data:\n",
            "Memory usage: 2082.54 MB\n",
            "\n",
            "Combined dataset info:\n",
            "Dimensions: FrozenMappingWarningOnValuesAccess({'valid_time': 2160, 'pressure_level': 1, 'latitude': 101, 'longitude': 237})\n",
            "Coordinates: ['number', 'valid_time', 'pressure_level', 'latitude', 'longitude']\n",
            "Variables: ['number', 'valid_time', 'pressure_level', 'latitude', 'longitude', 'q', 't', 'tp', 'slhf', 'sshf', 'ssrd', 'strd', 'u10', 'v10', 'd2m', 't2m', 'sp', 'tcc', 'stl1', 'blh']\n",
            "Pressure levels found: [850.]\n",
            "t2m dimensions: ('valid_time', 'latitude', 'longitude')\n",
            "Dataset size: 2160 time steps, 101 lat, 237 lon\n",
            "Input shape (with 4 channels): (2160, 101, 237, 4)\n",
            "X_train shape: (1727, 101, 237, 4)\n",
            "X_test shape: (432, 101, 237, 4)\n",
            "y_train shape: (1727, 101, 237)\n",
            "y_test shape: (432, 101, 237)\n",
            "Memory usage after loading data:\n",
            "Memory usage: 4064.77 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 205ms/step - loss: 2.2588 - mae: 0.3468 - val_loss: 1.0223 - val_mae: 0.5188 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.6660 - mae: 0.1453 - val_loss: 0.6569 - val_mae: 0.5202 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.3483 - mae: 0.1094 - val_loss: 0.5147 - val_mae: 0.5287 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.2171 - mae: 0.1086 - val_loss: 0.4349 - val_mae: 0.5337 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.1387 - mae: 0.0964 - val_loss: 0.3607 - val_mae: 0.5134 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.0929 - mae: 0.0879 - val_loss: 0.3039 - val_mae: 0.4831 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.0658 - mae: 0.0807 - val_loss: 0.2889 - val_mae: 0.4850 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.0585 - mae: 0.0979 - val_loss: 0.2364 - val_mae: 0.4431 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.0436 - mae: 0.0865 - val_loss: 0.2029 - val_mae: 0.4106 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.0398 - mae: 0.0887 - val_loss: 0.1807 - val_mae: 0.3907 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.0308 - mae: 0.0805 - val_loss: 0.1480 - val_mae: 0.3531 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.0251 - mae: 0.0731 - val_loss: 0.0987 - val_mae: 0.2795 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.0226 - mae: 0.0730 - val_loss: 0.0933 - val_mae: 0.2770 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - loss: 0.0215 - mae: 0.0739 - val_loss: 0.0977 - val_mae: 0.2873 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - loss: 0.0193 - mae: 0.0733 - val_loss: 0.0961 - val_mae: 0.2847 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.0169 - mae: 0.0683 - val_loss: 0.0658 - val_mae: 0.2329 - learning_rate: 0.0010\n",
            "Epoch 17/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.0157 - mae: 0.0691 - val_loss: 0.0352 - val_mae: 0.1543 - learning_rate: 0.0010\n",
            "Epoch 18/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.0134 - mae: 0.0646 - val_loss: 0.0306 - val_mae: 0.1413 - learning_rate: 0.0010\n",
            "Epoch 19/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.0124 - mae: 0.0641 - val_loss: 0.0272 - val_mae: 0.1334 - learning_rate: 0.0010\n",
            "Epoch 20/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.0107 - mae: 0.0614 - val_loss: 0.0228 - val_mae: 0.1192 - learning_rate: 0.0010\n",
            "Epoch 21/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.0098 - mae: 0.0604 - val_loss: 0.0170 - val_mae: 0.0943 - learning_rate: 0.0010\n",
            "Epoch 22/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.0087 - mae: 0.0574 - val_loss: 0.0090 - val_mae: 0.0607 - learning_rate: 0.0010\n",
            "Epoch 23/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 0.0073 - mae: 0.0543 - val_loss: 0.0086 - val_mae: 0.0622 - learning_rate: 0.0010\n",
            "Epoch 24/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 0.0063 - mae: 0.0518 - val_loss: 0.0074 - val_mae: 0.0562 - learning_rate: 0.0010\n",
            "Epoch 25/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 0.0061 - mae: 0.0514 - val_loss: 0.0063 - val_mae: 0.0504 - learning_rate: 0.0010\n",
            "Epoch 26/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 0.0059 - mae: 0.0500 - val_loss: 0.0053 - val_mae: 0.0463 - learning_rate: 0.0010\n",
            "Epoch 27/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0055 - mae: 0.0479 - val_loss: 0.0069 - val_mae: 0.0551 - learning_rate: 0.0010\n",
            "Epoch 28/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 0.0054 - mae: 0.0473 - val_loss: 0.0049 - val_mae: 0.0435 - learning_rate: 0.0010\n",
            "Epoch 29/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 0.0056 - mae: 0.0484 - val_loss: 0.0048 - val_mae: 0.0426 - learning_rate: 0.0010\n",
            "Epoch 30/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0054 - mae: 0.0475 - val_loss: 0.0078 - val_mae: 0.0615 - learning_rate: 0.0010\n",
            "Epoch 31/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0057 - mae: 0.0492 - val_loss: 0.0080 - val_mae: 0.0615 - learning_rate: 0.0010\n",
            "Epoch 32/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0051 - mae: 0.0466 - val_loss: 0.0053 - val_mae: 0.0469 - learning_rate: 0.0010\n",
            "Epoch 33/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0052 - mae: 0.0468 - val_loss: 0.0060 - val_mae: 0.0513 - learning_rate: 0.0010\n",
            "Epoch 34/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 0.0047 - mae: 0.0454 - val_loss: 0.0044 - val_mae: 0.0414 - learning_rate: 5.0000e-04\n",
            "Epoch 35/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 0.0046 - mae: 0.0452 - val_loss: 0.0040 - val_mae: 0.0405 - learning_rate: 5.0000e-04\n",
            "Epoch 36/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - loss: 0.0047 - mae: 0.0453 - val_loss: 0.0044 - val_mae: 0.0414 - learning_rate: 5.0000e-04\n",
            "Epoch 37/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0044 - mae: 0.0442 - val_loss: 0.0075 - val_mae: 0.0604 - learning_rate: 5.0000e-04\n",
            "Epoch 38/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0047 - mae: 0.0452 - val_loss: 0.0046 - val_mae: 0.0439 - learning_rate: 5.0000e-04\n",
            "Epoch 39/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0043 - mae: 0.0441 - val_loss: 0.0051 - val_mae: 0.0473 - learning_rate: 5.0000e-04\n",
            "Epoch 40/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0045 - mae: 0.0448 - val_loss: 0.0051 - val_mae: 0.0479 - learning_rate: 5.0000e-04\n",
            "Epoch 41/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - loss: 0.0043 - mae: 0.0445 - val_loss: 0.0042 - val_mae: 0.0435 - learning_rate: 2.5000e-04\n",
            "Epoch 42/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0041 - mae: 0.0441 - val_loss: 0.0042 - val_mae: 0.0431 - learning_rate: 2.5000e-04\n",
            "Epoch 43/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0041 - mae: 0.0438 - val_loss: 0.0047 - val_mae: 0.0466 - learning_rate: 2.5000e-04\n",
            "Epoch 44/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0040 - mae: 0.0434 - val_loss: 0.0045 - val_mae: 0.0448 - learning_rate: 2.5000e-04\n",
            "Epoch 45/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0042 - mae: 0.0443 - val_loss: 0.0055 - val_mae: 0.0512 - learning_rate: 2.5000e-04\n",
            "Epoch 46/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 0.0041 - mae: 0.0437 - val_loss: 0.0040 - val_mae: 0.0422 - learning_rate: 1.2500e-04\n",
            "Epoch 47/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0040 - mae: 0.0437 - val_loss: 0.0042 - val_mae: 0.0440 - learning_rate: 1.2500e-04\n",
            "Epoch 48/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 0.0039 - mae: 0.0432 - val_loss: 0.0037 - val_mae: 0.0410 - learning_rate: 1.2500e-04\n",
            "Epoch 49/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0039 - mae: 0.0433 - val_loss: 0.0039 - val_mae: 0.0417 - learning_rate: 1.2500e-04\n",
            "Epoch 50/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0039 - mae: 0.0433 - val_loss: 0.0053 - val_mae: 0.0499 - learning_rate: 1.2500e-04\n",
            "Model saved to temperature_cnn_model.keras\n",
            "Training history saved to training_history.csv\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step\n",
            "Spatial plot saved to prediction_plot_spatial.png\n",
            "Time series plot saved to prediction_plot_timeseries.png\n",
            "Difference plot saved to prediction_plot_difference.png\n",
            "Scatter plot saved to prediction_plot_scatter.png\n",
            "\n",
            "Prediction Metrics:\n",
            "Mean Absolute Error (MAE): 3.4579 K\n",
            "Root Mean Squared Error (RMSE): 4.7116 K\n",
            "Mean Bias Error (MBE): -1.0660 K\n",
            "Loss curve plot saved to prediction_plot_loss_curve.png\n",
            "Error histogram plot saved to prediction_plot_error_histogram.png\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0044 - mae: 0.0448\n",
            "Test MAE (normalized scale): 0.0410\n",
            "Sample prediction shape: (432, 101, 237)\n"
          ]
        }
      ]
    }
  ]
}