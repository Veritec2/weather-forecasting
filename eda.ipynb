{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e8874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import cfgrib\n",
    "import pygrib\n",
    "import os\n",
    "import time\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import netCDF4 as nc\n",
    "import datetime as dt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315e0bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# File paths\n",
    "file_path_train_a = \"../nc/accum.nc\"\n",
    "file_path_train_i = \"../nc/instant.nc\"\n",
    "file_path_train_p = \"../nc/pressure.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340db595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_netcdf(file_path):\n",
    "    \"\"\"\n",
    "    Explore the contents of a netCDF file and return basic information.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the netCDF file\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing information about the file\n",
    "    \"\"\"\n",
    "    print(f\"\\nExploring: {os.path.basename(file_path)}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Open the file\n",
    "    dataset = nc.Dataset(file_path, 'r')\n",
    "    \n",
    "    # Get dimensions\n",
    "    dimensions = {dim: len(dataset.dimensions[dim]) for dim in dataset.dimensions}\n",
    "    print(\"Dimensions:\")\n",
    "    for dim_name, dim_size in dimensions.items():\n",
    "        print(f\"  {dim_name}: {dim_size}\")\n",
    "    \n",
    "    # Get variables\n",
    "    variables = {}\n",
    "    print(\"\\nVariables:\")\n",
    "    for var_name in dataset.variables:\n",
    "        var = dataset.variables[var_name]\n",
    "        var_shape = var.shape\n",
    "        var_dims = var.dimensions\n",
    "        var_dtype = var.dtype\n",
    "        variables[var_name] = {\n",
    "            'shape': var_shape,\n",
    "            'dimensions': var_dims,\n",
    "            'dtype': var_dtype\n",
    "        }\n",
    "        \n",
    "        # Add attributes if available\n",
    "        if hasattr(var, 'units'):\n",
    "            variables[var_name]['units'] = var.units\n",
    "        if hasattr(var, 'long_name'):\n",
    "            variables[var_name]['long_name'] = var.long_name\n",
    "            \n",
    "        print(f\"  {var_name}: shape={var_shape}, dims={var_dims}, type={var_dtype}\")\n",
    "        \n",
    "        # Print some attributes if available\n",
    "        if hasattr(var, 'units'):\n",
    "            print(f\"    units: {var.units}\")\n",
    "        if hasattr(var, 'long_name'):\n",
    "            print(f\"    long_name: {var.long_name}\")\n",
    "    \n",
    "    # Get global attributes\n",
    "    global_attrs = {attr: getattr(dataset, attr) for attr in dataset.ncattrs()}\n",
    "    print(\"\\nGlobal Attributes:\")\n",
    "    for attr_name, attr_value in global_attrs.items():\n",
    "        print(f\"  {attr_name}: {attr_value}\")\n",
    "    \n",
    "    # Close the dataset\n",
    "    dataset.close()\n",
    "    \n",
    "    return {\n",
    "        'dimensions': dimensions,\n",
    "        'variables': variables,\n",
    "        'global_attributes': global_attrs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e22ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(explore_netcdf(file_path_train_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(explore_netcdf(file_path_train_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(explore_netcdf(file_path_train_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3e9427",
   "metadata": {},
   "source": [
    "# Our Variables\n",
    "| Variable | Shape               | Dimensions                                       | Data Type | Units       | Long Name                                      | Type          |\n",
    "|----------|---------------------|--------------------------------------------------|-----------|-------------|------------------------------------------------|---------------|\n",
    "| tp       | (2160, 101, 237)    | (valid_time, latitude, longitude)                | float32   | m           | Total precipitation                            | accumulated   |\n",
    "| slhf     | (2160, 101, 237)    | (valid_time, latitude, longitude)                | float32   | J m**-2     | Time-integrated surface latent heat net flux   | accumulated   |\n",
    "| sshf     | (2160, 101, 237)    | (valid_time, latitude, longitude)                | float32   | J m**-2     | Time-integrated surface sensible heat net flux | accumulated   |\n",
    "| ssrd     | (2160, 101, 237)    | (valid_time, latitude, longitude)                | float32   | J m**-2     | Surface short-wave (solar) radiation downwards | accumulated   |\n",
    "| strd     | (2160, 101, 237)    | (valid_time, latitude, longitude)                | float32   | J m**-2     | Surface long-wave (thermal) radiation downwards| accumulated   |\n",
    "| u10      | (2160, 101, 237)    | (valid_time, latitude, longitude)                | float32   | m s**-1     | 10 metre U wind component                      | instantaneous |\n",
    "| v10      | (2160, 101, 237)    | (valid_time, latitude, longitude)                | float32   | m s**-1     | 10 metre V wind component                      | instantaneous |\n",
    "| d2m      | (2160, 101, 237)    | (valid_time, latitude, longitude)                | float32   | K           | 2 metre dewpoint temperature                   | instantaneous |\n",
    "| t2m      | (2160, 101, 237)    | (valid_time, latitude, longitude)                | float32   | K           | 2 metre temperature                            | instantaneous |\n",
    "| sp       | (2160, 101, 237)    | (valid_time, latitude, longitude)                | float32   | Pa          | Surface pressure                               | instantaneous |\n",
    "| tcc      | (2160, 101, 237)    | (valid_time, latitude, longitude)                | float32   | (0 - 1)     | Total cloud cover                              | instantaneous |\n",
    "| stl1     | (2160, 101, 237)    | (valid_time, latitude, longitude)                | float32   | K           | Soil temperature level 1                       | instantaneous |\n",
    "| blh      | (2160, 101, 237)    | (valid_time, latitude, longitude)                | float32   | m           | Boundary layer height                          | instantaneous |\n",
    "| q        | (2160, 1, 101, 237) | (valid_time, pressure_level, latitude, longitude)| float32   | kg kg**-1   | Specific humidity                              | pressure      |\n",
    "| t        | (2160, 1, 101, 237) | (valid_time, pressure_level, latitude, longitude)| float32   | K           | Temperature                                    | pressure      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc8877",
   "metadata": {},
   "source": [
    "# Visual Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0d0a5a",
   "metadata": {},
   "source": [
    "## Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44e768f",
   "metadata": {},
   "source": [
    "### All Time Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df048e1",
   "metadata": {},
   "source": [
    "We begin by just taking a look at the distribution of our variables at a few chosen locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc8431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define file paths for all datasets\n",
    "file_path_train_i = \"f:\\\\weather_forecasting\\\\notebooks\\\\final project\\\\nc\\\\instant.nc\"\n",
    "file_path_train_a = \"f:\\\\weather_forecasting\\\\notebooks\\\\final project\\\\nc\\\\accum.nc\"\n",
    "file_path_train_p = \"f:\\\\weather_forecasting\\\\notebooks\\\\final project\\\\nc\\\\pressure.nc\"\n",
    "\n",
    "# Define feature sets for all datasets with their dataset names\n",
    "datasets = {\n",
    "    'instant': {\n",
    "        'file_path': file_path_train_i,\n",
    "        'features': ['t2m', 'd2m', 'stl1', 'sp', 'u10', 'v10', 'tcc', 'blh']\n",
    "    },\n",
    "    'accum': {\n",
    "        'file_path': file_path_train_a,\n",
    "        'features': ['tp', 'slhf', 'sshf', 'ssrd', 'strd']\n",
    "    },\n",
    "    'pressure': {\n",
    "        'file_path': file_path_train_p,\n",
    "        'features': ['t', 'q']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define cities with their coordinates (latitude, longitude)\n",
    "cities = [\n",
    "    (\"Los Angeles\", 34.05, -118.24),\n",
    "    (\"San Francisco\", 37.77, -122.42),\n",
    "    (\"Seattle\", 47.61, -122.33),\n",
    "    (\"Missoula\", 46.87, -113.99),\n",
    "    (\"Salt Lake City\", 40.76, -111.89),\n",
    "    (\"Denver\", 39.74, -104.99),\n",
    "    (\"Phoenix\", 33.45, -112.07),\n",
    "    (\"San Antonio\", 29.42, -98.49),\n",
    "    (\"New Orleans\", 29.95, -90.07),\n",
    "    (\"Kansas City\", 39.10, -94.58),\n",
    "    (\"Chicago\", 41.88, -87.63),\n",
    "    (\"New York City\", 40.71, -74.01),\n",
    "    (\"Boston\", 42.36, -71.06),\n",
    "    (\"Philadelphia\", 39.95, -75.17),\n",
    "    (\"Atlanta\", 33.75, -84.39),\n",
    "    (\"Miami\", 25.76, -80.19)\n",
    "]\n",
    "\n",
    "# Directory to save images (current working directory)\n",
    "image_dir = \".\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "# Function to extract time series data at a specific location\n",
    "def extract_time_series(xrds, var_name, dataset_name, lat, lon):\n",
    "    # If the dataset is 'pressure', select the 850 hPa level\n",
    "    if dataset_name == 'pressure':\n",
    "        dims_to_exclude = {'latitude', 'longitude', 'valid_time'}\n",
    "        pressure_dim = [dim for dim in xrds[var_name].dims if dim not in dims_to_exclude]\n",
    "        if pressure_dim:\n",
    "            pressure_dim = pressure_dim[0]\n",
    "            try:\n",
    "                xrds = xrds.sel(**{pressure_dim: 850})\n",
    "            except KeyError:\n",
    "                xrds = xrds.isel(**{pressure_dim: 0})\n",
    "\n",
    "    # Interpolate the variable at the given latitude and longitude\n",
    "    data = xrds[var_name].interp(latitude=lat, longitude=lon, method='nearest')\n",
    "\n",
    "    # Get the time steps and values\n",
    "    time_steps = range(len(data.valid_time))\n",
    "    values = data.values\n",
    "\n",
    "    return time_steps, values\n",
    "\n",
    "# Function to create a 4x4 subplot figure for a variable across all cities\n",
    "def create_subplot_time_series_plot(var_name, dataset_name, time_series_data):\n",
    "    # Create a 4x4 subplot grid\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(20, 20))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Get long name and units for the variable\n",
    "    xrds = xr.open_dataset(datasets[dataset_name]['file_path'], engine=\"netcdf4\")\n",
    "    long_name = xrds[var_name].attrs.get('long_name', var_name)\n",
    "    units = xrds[var_name].attrs.get('units', '')\n",
    "    xrds.close()\n",
    "\n",
    "    # Plot time series for each city in its subplot\n",
    "    for idx, ((city, lat, lon), (time_steps, values)) in enumerate(zip(cities, time_series_data)):\n",
    "        ax = axes[idx]\n",
    "        ax.plot(time_steps, values, color='g')  # Green line, no markers\n",
    "        ax.set_title(city, fontsize=10)\n",
    "        ax.set_xlabel('Time Step', fontsize=8)\n",
    "        ax.set_ylabel(f\"{long_name} ({units})\", fontsize=8)\n",
    "        ax.grid(True)\n",
    "        ax.tick_params(axis='both', labelsize=8)\n",
    "\n",
    "    # Adjust layout to minimize white space\n",
    "    plt.tight_layout(pad=2.0, w_pad=1.0, h_pad=1.0)\n",
    "\n",
    "    # Add a super title for the entire figure\n",
    "    fig.suptitle(f\"Time Series of {long_name} Across Cities\", fontsize=16, y=1.02)\n",
    "\n",
    "    # Save the plot\n",
    "    output_path = os.path.join(image_dir, f\"time_series_subplots_{var_name}.png\")\n",
    "    plt.savefig(output_path, bbox_inches=\"tight\", dpi=300)\n",
    "    print(f\"Time series subplots for {var_name} saved as {output_path}\")\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# Process each dataset and variable\n",
    "variables = []\n",
    "for dataset_name, info in datasets.items():\n",
    "    for var_name in info['features']:\n",
    "        variables.append((var_name, dataset_name))\n",
    "\n",
    "# Generate time series subplots for each variable\n",
    "for var_name, dataset_name in variables:\n",
    "    print(f\"\\nProcessing variable {var_name} from {dataset_name} dataset...\\n\")\n",
    "\n",
    "    # Load the dataset\n",
    "    xrds = xr.open_dataset(datasets[dataset_name]['file_path'], engine=\"netcdf4\")\n",
    "\n",
    "    # Extract time series data for each city\n",
    "    time_series_data = []\n",
    "    for city, lat, lon in cities:\n",
    "        time_steps, values = extract_time_series(xrds, var_name, dataset_name, lat, lon)\n",
    "        time_series_data.append((time_steps, values))\n",
    "\n",
    "    xrds.close()\n",
    "\n",
    "    # Create the 4x4 subplot figure\n",
    "    create_subplot_time_series_plot(var_name, dataset_name, time_series_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86dbc5d",
   "metadata": {},
   "source": [
    "### First Day & First Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391572e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define file paths for all datasets\n",
    "file_path_train_i = \"f:\\\\weather_forecasting\\\\notebooks\\\\final project\\\\nc\\\\instant.nc\"\n",
    "file_path_train_a = \"f:\\\\weather_forecasting\\\\notebooks\\\\final project\\\\nc\\\\accum.nc\"\n",
    "file_path_train_p = \"f:\\\\weather_forecasting\\\\notebooks\\\\final project\\\\nc\\\\pressure.nc\"\n",
    "\n",
    "# Define feature sets for all datasets with their dataset names\n",
    "datasets = {\n",
    "    'instant': {\n",
    "        'file_path': file_path_train_i,\n",
    "        'features': ['t2m', 'd2m', 'stl1', 'sp', 'u10', 'v10', 'tcc', 'blh']\n",
    "    },\n",
    "    'accum': {\n",
    "        'file_path': file_path_train_a,\n",
    "        'features': ['tp', 'slhf', 'sshf', 'ssrd', 'strd']\n",
    "    },\n",
    "    'pressure': {\n",
    "        'file_path': file_path_train_p,\n",
    "        'features': ['t', 'q']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define cities with their coordinates (latitude, longitude)\n",
    "cities = [\n",
    "    (\"Los Angeles\", 34.05, -118.24),\n",
    "    (\"San Francisco\", 37.77, -122.42),\n",
    "    (\"Seattle\", 47.61, -122.33),\n",
    "    (\"Missoula\", 46.87, -113.99),\n",
    "    (\"Salt Lake City\", 40.76, -111.89),\n",
    "    (\"Denver\", 39.74, -104.99),\n",
    "    (\"Phoenix\", 33.45, -112.07),\n",
    "    (\"San Antonio\", 29.42, -98.49),\n",
    "    (\"New Orleans\", 29.95, -90.07),\n",
    "    (\"Kansas City\", 39.10, -94.58),\n",
    "    (\"Chicago\", 41.88, -87.63),\n",
    "    (\"New York City\", 40.71, -74.01),\n",
    "    (\"Boston\", 42.36, -71.06),\n",
    "    (\"Philadelphia\", 39.95, -75.17),\n",
    "    (\"Atlanta\", 33.75, -84.39),\n",
    "    (\"Miami\", 25.76, -80.19)\n",
    "]\n",
    "\n",
    "# Directory to save images (current working directory)\n",
    "image_dir = \".\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "# Function to extract time series data at a specific location\n",
    "def extract_time_series(xrds, var_name, dataset_name, lat, lon, max_timesteps):\n",
    "    # If the dataset is 'pressure', select the 850 hPa level\n",
    "    if dataset_name == 'pressure':\n",
    "        dims_to_exclude = {'latitude', 'longitude', 'valid_time'}\n",
    "        pressure_dim = [dim for dim in xrds[var_name].dims if dim not in dims_to_exclude]\n",
    "        if pressure_dim:\n",
    "            pressure_dim = pressure_dim[0]\n",
    "            try:\n",
    "                xrds = xrds.sel(**{pressure_dim: 850})\n",
    "            except KeyError:\n",
    "                xrds = xrds.isel(**{pressure_dim: 0})\n",
    "\n",
    "    # Limit the data to the specified number of time steps\n",
    "    data = xrds[var_name].isel(valid_time=slice(0, max_timesteps))\n",
    "\n",
    "    # Interpolate the variable at the given latitude and longitude\n",
    "    data = data.interp(latitude=lat, longitude=lon, method='nearest')\n",
    "\n",
    "    # Get the time steps and values\n",
    "    time_steps = range(len(data.valid_time))\n",
    "    values = data.values\n",
    "\n",
    "    return time_steps, values\n",
    "\n",
    "# Function to create a 4x4 subplot figure for a variable across all cities\n",
    "def create_subplot_time_series_plot(var_name, dataset_name, time_series_data, max_timesteps, time_range_label):\n",
    "    # Create a 4x4 subplot grid\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(20, 20))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Get long name and units for the variable\n",
    "    xrds = xr.open_dataset(datasets[dataset_name]['file_path'], engine=\"netcdf4\")\n",
    "    long_name = xrds[var_name].attrs.get('long_name', var_name)\n",
    "    units = xrds[var_name].attrs.get('units', '')\n",
    "    xrds.close()\n",
    "\n",
    "    # Plot time series for each city in its subplot\n",
    "    for idx, ((city, lat, lon), (time_steps, values)) in enumerate(zip(cities, time_series_data)):\n",
    "        ax = axes[idx]\n",
    "        ax.plot(time_steps, values, color='g')  # Green line, no markers\n",
    "        ax.set_title(city, fontsize=10)\n",
    "        ax.set_xlabel('Time Step', fontsize=8)\n",
    "        ax.set_ylabel(f\"{long_name} ({units})\", fontsize=8)\n",
    "        ax.grid(True)\n",
    "        ax.tick_params(axis='both', labelsize=8)\n",
    "\n",
    "    # Adjust layout to minimize white space\n",
    "    plt.tight_layout(pad=2.0, w_pad=1.0, h_pad=1.0)\n",
    "\n",
    "    # Add a super title for the entire figure\n",
    "    fig.suptitle(f\"Time Series of {long_name} Across Cities ({time_range_label})\", fontsize=16, y=1.02)\n",
    "\n",
    "    # Save the plot\n",
    "    output_path = os.path.join(image_dir, f\"time_series_subplots_{var_name}_{max_timesteps}timesteps.png\")\n",
    "    plt.savefig(output_path, bbox_inches=\"tight\", dpi=300)\n",
    "    print(f\"Time series subplots for {var_name} ({time_range_label}) saved as {output_path}\")\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# Process each dataset and variable for a specific number of time steps\n",
    "def process_time_series_plots(max_timesteps, time_range_label):\n",
    "    variables = []\n",
    "    for dataset_name, info in datasets.items():\n",
    "        for var_name in info['features']:\n",
    "            variables.append((var_name, dataset_name))\n",
    "\n",
    "    # Generate time series subplots for each variable\n",
    "    for var_name, dataset_name in variables:\n",
    "        print(f\"\\nProcessing variable {var_name} from {dataset_name} dataset ({time_range_label})...\\n\")\n",
    "\n",
    "        # Load the dataset\n",
    "        xrds = xr.open_dataset(datasets[dataset_name]['file_path'], engine=\"netcdf4\")\n",
    "\n",
    "        # Extract time series data for each city\n",
    "        time_series_data = []\n",
    "        for city, lat, lon in cities:\n",
    "            time_steps, values = extract_time_series(xrds, var_name, dataset_name, lat, lon, max_timesteps)\n",
    "            time_series_data.append((time_steps, values))\n",
    "\n",
    "        xrds.close()\n",
    "\n",
    "        # Create the 4x4 subplot figure\n",
    "        create_subplot_time_series_plot(var_name, dataset_name, time_series_data, max_timesteps, time_range_label)\n",
    "\n",
    "# Run for 1 day (first 24 time steps)\n",
    "process_time_series_plots(max_timesteps=24, time_range_label=\"1 Day\")\n",
    "\n",
    "# Run for 1 week (first 168 time steps)\n",
    "process_time_series_plots(max_timesteps=168, time_range_label=\"1 Week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b925e50",
   "metadata": {},
   "source": [
    "## Spatial Grid Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c667cf",
   "metadata": {},
   "source": [
    "With some time series analysis under our belt we next want to a get a grasp of what the data actually looks like across the entire grid at on specific moment in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5974884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for all datasets\n",
    "file_path_train_i = \"f:\\\\weather_forecasting\\\\notebooks\\\\final project\\\\nc\\\\instant.nc\"\n",
    "file_path_train_a = \"f:\\\\weather_forecasting\\\\notebooks\\\\final project\\\\nc\\\\accum.nc\"\n",
    "file_path_train_p = \"f:\\\\weather_forecasting\\\\notebooks\\\\final project\\\\nc\\\\pressure.nc\"\n",
    "\n",
    "# Define feature sets for all datasets\n",
    "feature_sets = {\n",
    "    'instant': ['t2m', 'd2m', 'stl1', 'sp', 'u10', 'v10', 'tcc', 'blh'],\n",
    "    'accum': ['tp', 'slhf', 'sshf', 'ssrd', 'strd'],\n",
    "    'pressure': ['t', 'q']\n",
    "}\n",
    "\n",
    "# Define colormaps for variables\n",
    "colormaps = {\n",
    "    # instant dataset\n",
    "    't2m': 'coolwarm',\n",
    "    'd2m': 'coolwarm',\n",
    "    'stl1': 'coolwarm',\n",
    "    'sp': 'coolwarm',\n",
    "    'u10': 'terrain',\n",
    "    'v10': 'terrain',\n",
    "    'tcc': 'terrain',\n",
    "    'blh': 'terrain',\n",
    "    # accum dataset\n",
    "    'tp': 'coolwarm',\n",
    "    'slhf': 'coolwarm',\n",
    "    'sshf': 'coolwarm',\n",
    "    'ssrd': 'terrain',\n",
    "    'strd': 'terrain',\n",
    "    # pressure dataset\n",
    "    't': 'coolwarm',\n",
    "    'q': 'terrain'\n",
    "}\n",
    "\n",
    "# Time index to plot\n",
    "time_index = 0\n",
    "\n",
    "# Directory to save images (current working directory)\n",
    "image_dir = \".\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "# Function to generate and save individual plots\n",
    "def generate_individual_plots(file_path, dataset_name, features):\n",
    "    try:\n",
    "        # Load NetCDF file\n",
    "        xrds = xr.open_dataset(file_path, engine=\"netcdf4\")\n",
    "        print(f\"Successfully opened {dataset_name} dataset: {file_path}\")\n",
    "\n",
    "        # Loop over each variable in the feature set\n",
    "        for var_name in features:\n",
    "            try:\n",
    "                if var_name in xrds.variables:\n",
    "                    # Select data for the current variable and time step\n",
    "                    data_at_time = xrds[var_name].isel(valid_time=time_index)\n",
    "\n",
    "                    # Print the dimensions and shape of the data\n",
    "                    print(f\"{var_name} dimensions: {data_at_time.dims}\")\n",
    "                    print(f\"{var_name} data shape before selections: {data_at_time.shape}\")\n",
    "\n",
    "                    # If the dataset is 'pressure', select the 850 hPa level\n",
    "                    if dataset_name == 'pressure':\n",
    "                        # Identify the pressure level dimension\n",
    "                        dims_to_exclude = {'latitude', 'longitude', 'valid_time'}\n",
    "                        pressure_dim = [dim for dim in data_at_time.dims if dim not in dims_to_exclude]\n",
    "                        \n",
    "                        if pressure_dim:\n",
    "                            pressure_dim = pressure_dim[0]\n",
    "                            print(f\"Found pressure dimension '{pressure_dim}' for {var_name}\")\n",
    "                            # Print available pressure levels\n",
    "                            print(f\"Available {pressure_dim} values: {xrds[pressure_dim].values}\")\n",
    "                            # Select the 850 hPa level\n",
    "                            try:\n",
    "                                data_at_time = data_at_time.sel(**{pressure_dim: 850})\n",
    "                                print(f\"Selected 850 hPa level for {var_name} using dimension {pressure_dim}\")\n",
    "                            except KeyError:\n",
    "                                print(f\"Error: 850 hPa not found in {pressure_dim} for {var_name}. Using first level instead.\")\n",
    "                                data_at_time = data_at_time.isel(**{pressure_dim: 0})\n",
    "                        else:\n",
    "                            print(f\"Warning: No pressure level dimension found for {var_name} in pressure dataset\")\n",
    "\n",
    "                    # Print the shape of the data after selections\n",
    "                    print(f\"{var_name} data shape after selections: {data_at_time.shape}\")\n",
    "\n",
    "                    # Get long name and units from variable attributes\n",
    "                    long_name = xrds[var_name].attrs.get('long_name', var_name)\n",
    "                    units = xrds[var_name].attrs.get('units', '')\n",
    "                    title = f\"{long_name} ({units})\" if units else long_name\n",
    "\n",
    "                    # Check the data's latitude and longitude bounds\n",
    "                    lon = data_at_time.longitude\n",
    "                    lat = data_at_time.latitude\n",
    "                    lon_min, lon_max = float(lon.min()), float(lon.max())\n",
    "                    lat_min, lat_max = float(lat.min()), float(lat.max())\n",
    "                    print(f\"{var_name} bounds: lon [{lon_min}, {lon_max}], lat [{lat_min}, {lat_max}]\")\n",
    "\n",
    "                    # Create figure with Cartopy projection\n",
    "                    fig = plt.figure(figsize=(14, 8))\n",
    "                    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "                    # Create meshgrid for pcolormesh\n",
    "                    lon, lat = np.meshgrid(data_at_time.longitude, data_at_time.latitude)\n",
    "\n",
    "                    # Plot the data using pcolormesh\n",
    "                    im = ax.pcolormesh(\n",
    "                        lon, lat, data_at_time,\n",
    "                        cmap=colormaps[var_name],\n",
    "                        transform=ccrs.PlateCarree()\n",
    "                    )\n",
    "\n",
    "                    # Add continental US features\n",
    "                    ax.add_feature(cfeature.LAND, edgecolor='black')\n",
    "                    ax.add_feature(cfeature.COASTLINE)\n",
    "                    ax.add_feature(cfeature.BORDERS)\n",
    "                    ax.add_feature(cfeature.STATES, edgecolor='gray')\n",
    "\n",
    "                    # Set extent to match the data's bounds, with a very small buffer\n",
    "                    buffer = 0.1\n",
    "                    ax.set_extent(\n",
    "                        [lon_min - buffer, lon_max + buffer, lat_min - buffer, lat_max + buffer],\n",
    "                        crs=ccrs.PlateCarree()\n",
    "                    )\n",
    "\n",
    "                    # Force the axes to fill the entire figure (leave space for colorbar)\n",
    "                    ax.set_position([0, 0, 0.85, 1])\n",
    "\n",
    "                    # Add a manually positioned colorbar\n",
    "                    cbar = plt.colorbar(\n",
    "                        im, \n",
    "                        ax=ax, \n",
    "                        orientation='vertical',\n",
    "                        fraction=0.03,\n",
    "                        pad=0.01,\n",
    "                        shrink=0.95,\n",
    "                        anchor=(1, 0.5),\n",
    "                        aspect=30\n",
    "                    )\n",
    "                    cbar.set_label('')\n",
    "                    cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "                    # Add title with long name and units\n",
    "                    plt.title(title, pad=20, fontsize=16)\n",
    "\n",
    "                    # Remove all figure margins\n",
    "                    fig.set_tight_layout(True)\n",
    "                    fig.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "\n",
    "                    # Save the plot at higher DPI\n",
    "                    output_path = os.path.join(image_dir, f\"{dataset_name}_{var_name}_time{time_index}_plot.png\")\n",
    "                    plt.savefig(output_path, bbox_inches=None, dpi=400)\n",
    "                    plt.close(fig)\n",
    "                    print(f\"Successfully plotted and saved {var_name} for {dataset_name}\")\n",
    "                else:\n",
    "                    print(f\"Warning: Variable {var_name} not found in {dataset_name} dataset.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error plotting {var_name} for {dataset_name}: {str(e)}\")\n",
    "                plt.close()\n",
    "        return xrds\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening {dataset_name} dataset {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to combine plots into a figure\n",
    "def combine_plots(dataset_name, features, nrows, ncols, figsize, suffix):\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    plot_idx = 0\n",
    "    for var_name in features:\n",
    "        image_path = os.path.join(image_dir, f\"{dataset_name}_{var_name}_time{time_index}_plot.png\")\n",
    "        try:\n",
    "            if os.path.exists(image_path):\n",
    "                img = imread(image_path)\n",
    "                axes[plot_idx].imshow(img)\n",
    "                axes[plot_idx].axis('off')\n",
    "                plot_idx += 1\n",
    "            else:\n",
    "                print(f\"Warning: Image not found: {image_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {image_path}: {str(e)}\")\n",
    "\n",
    "    # Turn off unused subplots (if any)\n",
    "    for i in range(plot_idx, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Adjust layout to minimize white space\n",
    "    plt.tight_layout(pad=1.0, w_pad=0.5, h_pad=0.5)\n",
    "\n",
    "    # Save and display the figure\n",
    "    combined_output_path = os.path.join(image_dir, f\"combined_weather_plots_{dataset_name}_{suffix}.png\")\n",
    "    plt.savefig(combined_output_path, bbox_inches=\"tight\", dpi=300)\n",
    "    print(f\"Combined figure for {dataset_name} saved as {combined_output_path}\")\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# Process all datasets\n",
    "# Dataset 1: instant.nc (two 2x2 figures)\n",
    "xrds_i = generate_individual_plots(file_path_train_i, \"instant\", feature_sets['instant'])\n",
    "if xrds_i is not None:\n",
    "    # First 2x2 figure for the first four variables\n",
    "    combine_plots(\"instant\", feature_sets['instant'][:4], nrows=2, ncols=2, figsize=(28, 16), suffix=\"1\")\n",
    "    # Second 2x2 figure for the last four variables\n",
    "    combine_plots(\"instant\", feature_sets['instant'][4:], nrows=2, ncols=2, figsize=(28, 16), suffix=\"2\")\n",
    "    xrds_i.close()\n",
    "\n",
    "# Dataset 2: accum.nc (5x1 figure)\n",
    "xrds_a = generate_individual_plots(file_path_train_a, \"accum\", feature_sets['accum'])\n",
    "if xrds_a is not None:\n",
    "    combine_plots(\"accum\", feature_sets['accum'], nrows=5, ncols=1, figsize=(14, 40), suffix=\"all\")\n",
    "    xrds_a.close()\n",
    "\n",
    "# Dataset 3: pressure.nc (2x1 figure)\n",
    "xrds_p = generate_individual_plots(file_path_train_p, \"pressure\", feature_sets['pressure'])\n",
    "if xrds_p is not None:\n",
    "    combine_plots(\"pressure\", feature_sets['pressure'], nrows=2, ncols=1, figsize=(14, 16), suffix=\"all\")\n",
    "    xrds_p.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f353e9c",
   "metadata": {},
   "source": [
    "Let get a better look at what an entire day looks like (in 6 hour increments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fcc9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define file paths for all datasets\n",
    "file_path_train_i = \"f:\\\\weather_forecasting\\\\notebooks\\\\final project\\\\nc\\\\instant.nc\"\n",
    "file_path_train_a = \"f:\\\\weather_forecasting\\\\notebooks\\\\final project\\\\nc\\\\accum.nc\"\n",
    "file_path_train_p = \"f:\\\\weather_forecasting\\\\notebooks\\\\final project\\\\nc\\\\pressure.nc\"\n",
    "\n",
    "# Define feature sets for all datasets with their dataset names\n",
    "datasets = {\n",
    "    'instant': {\n",
    "        'file_path': file_path_train_i,\n",
    "        'features': ['t2m', 'd2m', 'stl1', 'sp', 'u10', 'v10', 'tcc', 'blh']\n",
    "    },\n",
    "    'accum': {\n",
    "        'file_path': file_path_train_a,\n",
    "        'features': ['tp', 'slhf', 'sshf', 'ssrd', 'strd']\n",
    "    },\n",
    "    'pressure': {\n",
    "        'file_path': file_path_train_p,\n",
    "        'features': ['t', 'q']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define colormaps for variables\n",
    "colormaps = {\n",
    "    't2m': 'coolwarm',\n",
    "    'd2m': 'coolwarm',\n",
    "    'stl1': 'coolwarm',\n",
    "    'sp': 'coolwarm',\n",
    "    'u10': 'terrain',\n",
    "    'v10': 'terrain',\n",
    "    'tcc': 'terrain',\n",
    "    'blh': 'terrain',\n",
    "    'tp': 'coolwarm',\n",
    "    'slhf': 'coolwarm',\n",
    "    'sshf': 'coolwarm',\n",
    "    'ssrd': 'terrain',\n",
    "    'strd': 'terrain',\n",
    "    't': 'coolwarm',\n",
    "    'q': 'terrain'\n",
    "}\n",
    "\n",
    "# Time indices to plot (1st, 6th, 12th, 18th time steps, zero-based)\n",
    "time_indices = [0, 5, 11, 17]\n",
    "\n",
    "# Directory to save images (current working directory)\n",
    "image_dir = \".\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "# Function to generate and save individual plots\n",
    "def generate_individual_plots(file_path, dataset_name, features, time_idx):\n",
    "    # Load NetCDF file\n",
    "    xrds = xr.open_dataset(file_path, engine=\"netcdf4\")\n",
    "\n",
    "    # Loop over each variable in the feature set\n",
    "    for var_name in features:\n",
    "        # Select data for the current variable and time step\n",
    "        data_at_time = xrds[var_name].isel(valid_time=time_idx)\n",
    "\n",
    "        # If the dataset is 'pressure', select the 850 hPa level\n",
    "        if dataset_name == 'pressure':\n",
    "            dims_to_exclude = {'latitude', 'longitude', 'valid_time'}\n",
    "            pressure_dim = [dim for dim in data_at_time.dims if dim not in dims_to_exclude]\n",
    "            if pressure_dim:\n",
    "                pressure_dim = pressure_dim[0]\n",
    "                try:\n",
    "                    data_at_time = data_at_time.sel(**{pressure_dim: 850})\n",
    "                except KeyError:\n",
    "                    data_at_time = data_at_time.isel(**{pressure_dim: 0})\n",
    "\n",
    "        # Get long name and units from variable attributes\n",
    "        long_name = xrds[var_name].attrs.get('long_name', var_name)\n",
    "        units = xrds[var_name].attrs.get('units', '')\n",
    "        title = f\"{long_name} ({units}) at Time Step {time_idx + 1}\"\n",
    "\n",
    "        # Create figure with Cartopy projection\n",
    "        fig = plt.figure(figsize=(14, 8))\n",
    "        ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "        # Create meshgrid for pcolormesh\n",
    "        lon, lat = np.meshgrid(data_at_time.longitude, data_at_time.latitude)\n",
    "\n",
    "        # Plot the data using pcolormesh\n",
    "        im = ax.pcolormesh(\n",
    "            lon, lat, data_at_time,\n",
    "            cmap=colormaps[var_name],\n",
    "            transform=ccrs.PlateCarree()\n",
    "        )\n",
    "\n",
    "        # Add continental US features\n",
    "        ax.add_feature(cfeature.LAND, edgecolor='black')\n",
    "        ax.add_feature(cfeature.COASTLINE)\n",
    "        ax.add_feature(cfeature.BORDERS)\n",
    "        ax.add_feature(cfeature.STATES, edgecolor='gray')\n",
    "\n",
    "        # Set extent to match the data's bounds, with a very small buffer\n",
    "        buffer = 0.1\n",
    "        lon_min, lon_max = float(data_at_time.longitude.min()), float(data_at_time.longitude.max())\n",
    "        lat_min, lat_max = float(data_at_time.latitude.min()), float(data_at_time.latitude.max())\n",
    "        ax.set_extent(\n",
    "            [lon_min - buffer, lon_max + buffer, lat_min - buffer, lat_max + buffer],\n",
    "            crs=ccrs.PlateCarree()\n",
    "        )\n",
    "\n",
    "        # Force the axes to fill the entire figure (leave space for colorbar)\n",
    "        ax.set_position([0, 0, 0.85, 1])\n",
    "\n",
    "        # Add a manually positioned colorbar\n",
    "        cbar = plt.colorbar(\n",
    "            im, \n",
    "            ax=ax, \n",
    "            orientation='vertical',\n",
    "            fraction=0.03,\n",
    "            pad=0.01,\n",
    "            shrink=0.95,\n",
    "            anchor=(1, 0.5),\n",
    "            aspect=30\n",
    "        )\n",
    "        cbar.set_label('')\n",
    "        cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "        # Add title with long name, units, and time step\n",
    "        plt.title(title, pad=20, fontsize=16)\n",
    "\n",
    "        # Remove all figure margins\n",
    "        fig.set_tight_layout(True)\n",
    "        fig.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "\n",
    "        # Save the plot at higher DPI\n",
    "        output_path = os.path.join(image_dir, f\"{dataset_name}_{var_name}_time{time_idx}_plot.png\")\n",
    "        plt.savefig(output_path, bbox_inches=None, dpi=400)\n",
    "        plt.close(fig)\n",
    "\n",
    "    return xrds\n",
    "\n",
    "# Function to create 2x2 figures for each variable across all time steps\n",
    "def create_variable_comparison_plots(variables):\n",
    "    for var_name, dataset_name in variables:\n",
    "        print(f\"\\nCreating comparison plot for {var_name} across all time steps...\\n\")\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(28, 16))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for idx, time_idx in enumerate(time_indices):\n",
    "            image_path = os.path.join(image_dir, f\"{dataset_name}_{var_name}_time{time_idx}_plot.png\")\n",
    "            if os.path.exists(image_path):\n",
    "                img = imread(image_path)\n",
    "                axes[idx].imshow(img)\n",
    "                axes[idx].axis('off')\n",
    "            else:\n",
    "                print(f\"Warning: Image not found: {image_path}\")\n",
    "\n",
    "        # Adjust layout to minimize white space\n",
    "        plt.tight_layout(pad=1.0, w_pad=0.5, h_pad=0.5)\n",
    "\n",
    "        # Save and display the figure\n",
    "        combined_output_path = os.path.join(image_dir, f\"variable_comparison_{var_name}_across_timesteps.png\")\n",
    "        plt.savefig(combined_output_path, bbox_inches=\"tight\", dpi=300)\n",
    "        print(f\"Combined figure for {var_name} saved as {combined_output_path}\")\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "# Process all datasets for each time step to generate individual plots\n",
    "for time_idx in time_indices:\n",
    "    print(f\"\\nProcessing time step {time_idx + 1} (index {time_idx})...\\n\")\n",
    "\n",
    "    # Dataset 1: instant.nc\n",
    "    xrds_i = generate_individual_plots(datasets['instant']['file_path'], \"instant\", datasets['instant']['features'], time_idx)\n",
    "    if xrds_i is not None:\n",
    "        xrds_i.close()\n",
    "\n",
    "    # Dataset 2: accum.nc\n",
    "    xrds_a = generate_individual_plots(datasets['accum']['file_path'], \"accum\", datasets['accum']['features'], time_idx)\n",
    "    if xrds_a is not None:\n",
    "        xrds_a.close()\n",
    "\n",
    "    # Dataset 3: pressure.nc\n",
    "    xrds_p = generate_individual_plots(datasets['pressure']['file_path'], \"pressure\", datasets['pressure']['features'], time_idx)\n",
    "    if xrds_p is not None:\n",
    "        xrds_p.close()\n",
    "\n",
    "# Create 2x2 figures for each variable across all time steps\n",
    "variables = []\n",
    "for dataset_name, info in datasets.items():\n",
    "    for var_name in info['features']:\n",
    "        variables.append((var_name, dataset_name))\n",
    "\n",
    "create_variable_comparison_plots(variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f7264",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0877b3",
   "metadata": {},
   "source": [
    "With a better understanding our data set we turn our attention towards data processing and creating our tensors. The expver is a key used to identify the specific model version and will not be of importance to us. After dropping this from our dataset we merge the three datasets, perform some basic conversions from Kelvin to Fahrenheit, and add the temperature spread variable t_spread_f. The new spread variable has two purposes: the first is to act as a basic measure of the relative humidity. It will not be perfect given the actual equation: RH = 100 * (exp((17.625 * d2m) / (243.04 + d2m)) / exp((17.625 * t2m) / (243.04 + t2m))), but it will help. The second purpose of introducing this new variable is to replace the dewpoint tempertaure feature d2m altogether to avoid any collinearity concerns which arise from using both the air temperature and dewpoint temperature to train our model. We also drop all of the original temperature variables (in Kelvin) and save the new dataset. With the goal being to train a model that will predict the temperature at the next hour we construct our input tensor using all time steps up to the last and our target tensor using all time steps but the first. In this way the model will use time step t to predict time step t+1. To achieve this we convert each time step into a numpy array and then a pytorch tensor object. Lastly, we normalize our tensors and save them for easy access when constructing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc2aa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# File paths\n",
    "file_path_train_a = \"../nc/accum.nc\"\n",
    "file_path_train_i = \"../nc/instant.nc\"\n",
    "file_path_train_p = \"../nc/pressure.nc\"\n",
    "\n",
    "# Open datasets with larger chunks\n",
    "# Training Data (January - March 2025)\n",
    "start = time.time()\n",
    "ds_a_train = xr.open_dataset(file_path_train_a, chunks={'valid_time': 100})\n",
    "ds_i_train = xr.open_dataset(file_path_train_i, chunks={'valid_time': 100})\n",
    "ds_p_train = xr.open_dataset(file_path_train_p, chunks={'valid_time':100})\n",
    "# Drop expver if present\n",
    "if 'expver' in ds_a_train.coords:\n",
    "    ds_a_train = ds_a_train.drop_vars('expver')\n",
    "if 'expver' in ds_i_train.coords:\n",
    "    ds_i_train = ds_i_train.drop_vars('expver')\n",
    "if 'expver' in ds_p_train.coords:\n",
    "    ds_p_train = ds_p_train.drop_vars('expver')\n",
    "print(f\"Training datasets opened in {time.time() - start:.2f}s: ds_i_train shape {ds_i_train.dims}, ds_a_train shape {ds_a_train.dims}, ds_p_train shape {ds_p_train.dims}\")\n",
    "\n",
    "# Define variable names\n",
    "i_vars = ['t2m', 'd2m', 'tcc', 'sp', 'u10', 'v10', 'stl1', 'blh']\n",
    "a_vars = ['tp', 'sshf', 'slhf', 'ssrd', 'strd']\n",
    "p_vars = ['t', 'q']\n",
    "print(f\"Variables defined: instant {i_vars}, accum {a_vars}, pressure {p_vars}\")\n",
    "\n",
    "# Merge datasets\n",
    "# Training Data\n",
    "start = time.time()\n",
    "ds_train_merge = xr.merge([ds_i_train[i_vars], ds_a_train[a_vars], ds_p_train[p_vars]])\n",
    "print(f\"Training datasets merged in {time.time() - start:.2f}s: shape {ds_train_merge.dims}, dtype {ds_train_merge[i_vars[0]].dtype}\")\n",
    "\n",
    "# Convert to Fahrenheit\n",
    "def kelvin_to_fahrenheit(temp_k):\n",
    "    temp_f = (temp_k - 273.15) * 9/5 + 32\n",
    "    return temp_f\n",
    "\n",
    "# Converting to fahrenheit & adding temperature spread\n",
    "start = time.time()\n",
    "ds_train_merge = ds_train_merge.assign(t2m_f=lambda ds: kelvin_to_fahrenheit(ds['t2m']))\n",
    "ds_train_merge = ds_train_merge.assign(d2m_f=lambda ds: kelvin_to_fahrenheit(ds['d2m']))\n",
    "ds_train_merge = ds_train_merge.assign(t_f=lambda ds: kelvin_to_fahrenheit(ds['t']))\n",
    "ds_train_merge = ds_train_merge.assign(t_spread_f=lambda ds: ds['t2m_f']-ds['d2m_f'])\n",
    "print(f\"Converting to F in {time.time() - start:.2f}\")\n",
    "\n",
    "# Dropping the original temperature variables as well as d2m_f\n",
    "ds_train_merge = ds_train_merge.drop_vars(['t2m', 'd2m', 'd2m_f', 't'])\n",
    "\n",
    "# Update variables\n",
    "i_vars = ['t2m_f', 't_spread_f', 'tcc', 'sp', 'u10', 'v10', 'stl1', 'blh']\n",
    "a_vars = ['tp', 'sshf', 'slhf', 'ssrd', 'strd']\n",
    "p_vars = ['t_f', 'q']\n",
    "all_vars = a_vars+i_vars+p_vars\n",
    "\n",
    "# Save intermediates\n",
    "start = time.time()\n",
    "ds_train_merge.to_netcdf('f:/weather_forecasting/ds_train.nc')\n",
    "print(f\"Saved intermediates in {time.time() - start:.2f}s: train shape {ds_train_merge.dims}\")\n",
    "\n",
    "# Stack spatial dimensions\n",
    "start = time.time()\n",
    "ds_train_stacked = ds_train_merge[all_vars].stack(node=('latitude', 'longitude'))\n",
    "print(f\"Stacked spatial dims in {time.time() - start:.2f}s: train shape {ds_train_stacked.dims}\")\n",
    "\n",
    "# Create inputs and targets (all time steps)\n",
    "start = time.time()\n",
    "# Training\n",
    "total_train_time_steps = ds_train_stacked.dims['valid_time']\n",
    "print(f\"Total training time steps: {total_train_time_steps}\")\n",
    "train_inputs = ds_train_stacked.isel(valid_time=slice(0, total_train_time_steps - 1))\n",
    "train_targets = ds_train_stacked.isel(valid_time=slice(1, total_train_time_steps))\n",
    "\n",
    "# Tensor build\n",
    "# Convert to numpy array\n",
    "start = time.time()\n",
    "train_inputs_array = train_inputs.to_array().values.transpose(1, 2, 0, 3)\n",
    "train_targets_array = train_targets.to_array().values.transpose(1, 2, 0, 3)\n",
    "print(f\"Converted to arrays in {time.time() - start:.2f}s: train inputs shape {train_inputs_array.shape}, train targets shape {train_targets_array.shape}, dtype {train_inputs_array.dtype}\")\n",
    "\n",
    "# Convert to pytorch tensors\n",
    "start = time.time()\n",
    "train_inputs_tensor = t.tensor(train_inputs_array, dtype=t.float32).to(device)\n",
    "train_targets_tensor = t.tensor(train_targets_array, dtype=t.float32).to(device)\n",
    "print(f\"Tensors built in {time.time() - start:.2f}s: train inputs shape {train_inputs_tensor.shape}, train targets shape {train_targets_tensor.shape}, device {train_inputs_tensor.device}\")\n",
    "\n",
    "# Normalize tensors (using mean and std from the training dataset only)\n",
    "start = time.time()\n",
    "train_inputs_mean = train_inputs_tensor.mean(dim=(0, 1), keepdim=True)\n",
    "train_inputs_std = train_inputs_tensor.std(dim=(0, 1), keepdim=True)\n",
    "train_targets_mean = train_targets_tensor.mean(dim=(0, 1), keepdim=True)\n",
    "train_targets_std = train_targets_tensor.std(dim=(0, 1), keepdim=True)\n",
    "\n",
    "# Normalize training tensors\n",
    "train_inputs_tensor = (train_inputs_tensor - train_inputs_mean) / (train_inputs_std + 1e-8)\n",
    "train_targets_tensor = (train_targets_tensor - train_targets_mean) / (train_targets_std + 1e-8)\n",
    "\n",
    "# Save normalized tensors\n",
    "start = time.time()\n",
    "t.save(train_inputs_tensor.cpu(), 'f:/weather_forecasting/notebooks/final project/tensors/train_inputs_norm.pt')\n",
    "t.save(train_targets_tensor.cpu(), 'f:/weather_forecasting/notebooks/final project/tensors/train_targets_norm.pt')\n",
    "print(f\"Normalized tensors saved in {time.time() - start:.2f}s to f:/weather_forecasting/notebooks/final project/tensors\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
